import os
from typing import List, Union, Set, Optional
import hashlib

from tqdm import tqdm

# from deepsearcher.configuration import embedding_model, vector_db, file_loader
from deepsearcher import configuration
from deepsearcher.loader.splitter import split_docs_to_chunks
from deepsearcher.utils import log


def _generate_document_id(file_path: str, content_hash: str = None) -> str:
    """
    ç”Ÿæˆæ–‡æ¡£å”¯ä¸€ID
    
    Args:
        file_path: æ–‡ä»¶è·¯å¾„
        content_hash: å†…å®¹å“ˆå¸Œå€¼ï¼ˆå¯é€‰ï¼‰
    
    Returns:
        æ–‡æ¡£å”¯ä¸€ID
    """
    if content_hash:
        return f"{os.path.basename(file_path)}_{content_hash[:8]}"
    else:
        return f"{os.path.basename(file_path)}_{hash(file_path) % 1000000}"


def _get_existing_document_ids(vector_db, collection_name: str, embedding_model) -> Set[str]:
    """
    è·å–ç°æœ‰æ–‡æ¡£IDé›†åˆ
    
    Args:
        vector_db: å‘é‡æ•°æ®åº“å®ä¾‹
        collection_name: é›†åˆåç§°
        embedding_model: åµŒå…¥æ¨¡å‹å®ä¾‹
        
    Returns:
        ç°æœ‰æ–‡æ¡£IDçš„é›†åˆ
    """
    try:
        # æ£€æŸ¥é›†åˆæ˜¯å¦å­˜åœ¨
        if hasattr(vector_db, 'client') and hasattr(vector_db.client, 'has_collection'):
            if not vector_db.client.has_collection(collection_name):
                log.color_print(f"Collection [{collection_name}] does not exist, will create new collection")
                return set()
        
        # åˆ›å»ºæµ‹è¯•å‘é‡è¿›è¡Œæœç´¢
        test_vector = embedding_model.embed_query("test document")
        
        # æœç´¢ç°æœ‰æ–‡æ¡£ï¼ˆä½¿ç”¨å¤§çš„top_kå€¼å°è¯•è·å–æ‰€æœ‰æ–‡æ¡£ï¼‰
        try:
            results = vector_db.search_data(
                collection=collection_name,
                vector=test_vector,
                top_k=10000  # å°è¯•è·å–å°½å¯èƒ½å¤šçš„æ–‡æ¡£
            )
            
            # æå–æ–‡æ¡£ID
            existing_ids = set()
            for result in results:
                if hasattr(result, 'metadata') and result.metadata:
                    doc_id = result.metadata.get('document_id')
                    if doc_id:
                        existing_ids.add(doc_id)
            
            log.color_print(f"Found {len(existing_ids)} existing documents in collection [{collection_name}]")
            return existing_ids
            
        except Exception as e:
            log.warning(f"Cannot retrieve existing document IDs: {e}")
            return set()
            
    except Exception as e:
        log.warning(f"Failed to get existing document IDs: {e}")
        return set()


def _filter_new_chunks(chunks: List, existing_ids: Set[str]) -> List:
    """
    è¿‡æ»¤å‡ºæ–°çš„æ–‡æ¡£å—ï¼ˆä¸åœ¨ç°æœ‰IDé›†åˆä¸­çš„ï¼‰
    
    Args:
        chunks: æ‰€æœ‰æ–‡æ¡£å—åˆ—è¡¨
        existing_ids: ç°æœ‰æ–‡æ¡£IDé›†åˆ
        
    Returns:
        æ–°æ–‡æ¡£å—åˆ—è¡¨
    """
    if not existing_ids:
        return chunks
    
    new_chunks = []
    for chunk in chunks:
        doc_id = chunk.metadata.get('document_id')
        if doc_id and doc_id not in existing_ids:
            new_chunks.append(chunk)
        elif not doc_id:
            # å¦‚æœæ²¡æœ‰document_idï¼Œä¹Ÿè®¤ä¸ºæ˜¯æ–°æ–‡æ¡£
            new_chunks.append(chunk)
    
    return new_chunks


def load_from_local_files(
    paths_or_directory: Union[str, List[str]],
    collection_name: str = None,
    collection_description: str = None,
    force_new_collection: bool = False,
    incremental_update: bool = True,  # æ–°å¢ï¼šå¯ç”¨å¢é‡æ›´æ–°
    chunk_size: int = 1500,
    chunk_overlap: int = 100,
    batch_size: int = 256,
):
    """
    Load knowledge from local files or directories into the vector database with incremental update support.

    This function processes files from the specified paths or directories,
    splits them into chunks, embeds the chunks, and stores them in the vector database.
    Now supports incremental updates to avoid re-processing existing documents.

    Args:
        paths_or_directory: A single path or a list of paths to files or directories to load.
        collection_name: Name of the collection to store the data in. If None, uses the default collection.
        collection_description: Description of the collection. If None, no description is set.
        force_new_collection: If True, drops the existing collection and creates a new one.
        incremental_update: If True, only process new documents not already in the collection.
        chunk_size: Size of each chunk in characters.
        chunk_overlap: Number of characters to overlap between chunks.
        batch_size: Number of chunks to process at once during embedding.

    Raises:
        FileNotFoundError: If any of the specified paths do not exist.
    """
    vector_db = configuration.vector_db
    if collection_name is None:
        collection_name = vector_db.default_collection
    collection_name = collection_name.replace(" ", "_").replace("-", "_")
    embedding_model = configuration.embedding_model
    file_loader = configuration.file_loader
    
    # æ£€æŸ¥ç°æœ‰æ–‡æ¡£IDï¼ˆå¦‚æœå¯ç”¨å¢é‡æ›´æ–°ä¸”ä¸å¼ºåˆ¶é‡å»ºï¼‰
    existing_ids = set()
    if incremental_update and not force_new_collection:
        log.color_print("ğŸ” Checking existing documents for incremental update...")
        existing_ids = _get_existing_document_ids(vector_db, collection_name, embedding_model)
    
    # åˆå§‹åŒ–é›†åˆ
    vector_db.init_collection(
        dim=embedding_model.dimension,
        collection=collection_name,
        description=collection_description,
        force_new_collection=force_new_collection,
    )
    
    if isinstance(paths_or_directory, str):
        paths_or_directory = [paths_or_directory]
    
    all_docs = []
    for path in tqdm(paths_or_directory, desc="Loading files"):
        if not os.path.exists(path):
            raise FileNotFoundError(f"Error: File or directory '{path}' does not exist.")
        if os.path.isdir(path):
            docs = file_loader.load_directory(path)
        else:
            docs = file_loader.load_file(path)
        
        # ä¸ºæ¯ä¸ªæ–‡æ¡£æ·»åŠ document_idåˆ°metadataä¸­
        for doc in docs:
            if hasattr(doc, 'metadata') and doc.metadata is not None:
                # ç”ŸæˆåŸºäºæ–‡ä»¶è·¯å¾„å’Œå†…å®¹çš„å”¯ä¸€ID
                content_hash = hashlib.md5(doc.page_content.encode('utf-8')).hexdigest()
                doc_id = _generate_document_id(path, content_hash)
                doc.metadata['document_id'] = doc_id
                doc.metadata['source_file'] = path
        
        all_docs.extend(docs)
    
    log.color_print(f"Loaded {len(all_docs)} documents from {len(paths_or_directory)} paths")
    
    # åˆ†å—å¤„ç†
    chunks = split_docs_to_chunks(
        all_docs,
        chunk_size=chunk_size,
        chunk_overlap=chunk_overlap,
    )
    
    # è¿‡æ»¤æ–°æ–‡æ¡£å—ï¼ˆå¦‚æœå¯ç”¨å¢é‡æ›´æ–°ï¼‰
    if incremental_update and existing_ids:
        original_count = len(chunks)
        chunks = _filter_new_chunks(chunks, existing_ids)
        log.color_print(f"Incremental update: {len(chunks)}/{original_count} chunks are new")
        
        if not chunks:
            log.color_print("âœ… No new documents to process. Collection is up to date.")
            return
    
    # ç”ŸæˆåµŒå…¥å‘é‡
    log.color_print(f"Generating embeddings for {len(chunks)} chunks...")
    chunks = embedding_model.embed_chunks(chunks, batch_size=batch_size)
    
    # æ’å…¥æ•°æ®
    log.color_print(f"Inserting {len(chunks)} chunks into collection [{collection_name}]...")
    vector_db.insert_data(collection=collection_name, chunks=chunks)
    
    log.color_print(f"âœ… Successfully loaded {len(chunks)} chunks into vector database!")


def load_from_website(
    urls: Union[str, List[str]],
    collection_name: str = None,
    collection_description: str = None,
    force_new_collection: bool = False,
    incremental_update: bool = True,  # æ–°å¢ï¼šå¯ç”¨å¢é‡æ›´æ–°
    chunk_size: int = 1500,
    chunk_overlap: int = 100,
    batch_size: int = 256,
    **crawl_kwargs,
):
    """
    Load knowledge from websites into the vector database with incremental update support.

    This function crawls the specified URLs, processes the content,
    splits it into chunks, embeds the chunks, and stores them in the vector database.
    Now supports incremental updates to avoid re-processing existing documents.

    Args:
        urls: A single URL or a list of URLs to crawl.
        collection_name: Name of the collection to store the data in. If None, uses the default collection.
        collection_description: Description of the collection. If None, no description is set.
        force_new_collection: If True, drops the existing collection and creates a new one.
        incremental_update: If True, only process new documents not already in the collection.
        chunk_size: Size of each chunk in characters.
        chunk_overlap: Number of characters to overlap between chunks.
        batch_size: Number of chunks to process at once during embedding.
        **crawl_kwargs: Additional keyword arguments to pass to the web crawler.
    """
    if isinstance(urls, str):
        urls = [urls]
    vector_db = configuration.vector_db
    if collection_name is None:
        collection_name = vector_db.default_collection
    collection_name = collection_name.replace(" ", "_").replace("-", "_")
    embedding_model = configuration.embedding_model
    web_crawler = configuration.web_crawler

    # æ£€æŸ¥ç°æœ‰æ–‡æ¡£IDï¼ˆå¦‚æœå¯ç”¨å¢é‡æ›´æ–°ä¸”ä¸å¼ºåˆ¶é‡å»ºï¼‰
    existing_ids = set()
    if incremental_update and not force_new_collection:
        log.color_print("ğŸ” Checking existing documents for incremental update...")
        existing_ids = _get_existing_document_ids(vector_db, collection_name, embedding_model)

    # åˆå§‹åŒ–é›†åˆ
    vector_db.init_collection(
        dim=embedding_model.dimension,
        collection=collection_name,
        description=collection_description,
        force_new_collection=force_new_collection,
    )

    # çˆ¬å–ç½‘é¡µå†…å®¹
    all_docs = web_crawler.crawl_urls(urls, **crawl_kwargs)
    
    # ä¸ºæ¯ä¸ªæ–‡æ¡£æ·»åŠ document_idåˆ°metadataä¸­
    for doc in all_docs:
        if hasattr(doc, 'metadata') and doc.metadata is not None:
            # ç”ŸæˆåŸºäºURLå’Œå†…å®¹çš„å”¯ä¸€ID
            url = doc.metadata.get('source', 'unknown_url')
            content_hash = hashlib.md5(doc.page_content.encode('utf-8')).hexdigest()
            doc_id = _generate_document_id(url, content_hash)
            doc.metadata['document_id'] = doc_id
            doc.metadata['source_url'] = url

    log.color_print(f"Crawled {len(all_docs)} documents from {len(urls)} URLs")

    # åˆ†å—å¤„ç†
    chunks = split_docs_to_chunks(
        all_docs,
        chunk_size=chunk_size,
        chunk_overlap=chunk_overlap,
    )
    
    # è¿‡æ»¤æ–°æ–‡æ¡£å—ï¼ˆå¦‚æœå¯ç”¨å¢é‡æ›´æ–°ï¼‰
    if incremental_update and existing_ids:
        original_count = len(chunks)
        chunks = _filter_new_chunks(chunks, existing_ids)
        log.color_print(f"Incremental update: {len(chunks)}/{original_count} chunks are new")
        
        if not chunks:
            log.color_print("âœ… No new documents to process. Collection is up to date.")
            return

    # ç”ŸæˆåµŒå…¥å‘é‡
    log.color_print(f"Generating embeddings for {len(chunks)} chunks...")
    chunks = embedding_model.embed_chunks(chunks, batch_size=batch_size)
    
    # æ’å…¥æ•°æ®
    log.color_print(f"Inserting {len(chunks)} chunks into collection [{collection_name}]...")
    vector_db.insert_data(collection=collection_name, chunks=chunks)
    
    log.color_print(f"âœ… Successfully loaded {len(chunks)} chunks into vector database!")
