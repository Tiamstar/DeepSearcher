#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
åä¸ºRAGå¢å¼ºç‰ˆåœ¨çº¿æœç´¢æ¨¡å—
ä¸“æ³¨äºä½¿ç”¨FireCrawlçš„æ™ºèƒ½æœç´¢åŠŸèƒ½ï¼Œç§»é™¤åŸºç¡€æœç´¢å¼•æ“ä¾èµ–
"""

import logging
import time
import os
import random
from typing import List, Dict, Any, Optional, Tuple
from langchain_core.documents import Document
from firecrawl import FirecrawlApp, ScrapeOptions

# å¯¼å…¥DeepSearcherçš„LLMé…ç½®
from deepsearcher.configuration import llm, embedding_model, vector_db

logger = logging.getLogger(__name__)

class EnhancedOnlineSearchEngine:
    """
    å¢å¼ºç‰ˆåœ¨çº¿æœç´¢å¼•æ“
    ä¸“é—¨ä½¿ç”¨FireCrawlçš„æ™ºèƒ½æœç´¢åŠŸèƒ½ï¼Œé’ˆå¯¹åä¸ºæ–‡æ¡£è¿›è¡Œä¼˜åŒ–
    """
    
    def __init__(self, 
                 max_search_results: int = 10,
                 max_sub_queries: int = 5,
                 chunk_size: int = 1000,
                 chunk_overlap: int = 200):
        """
        åˆå§‹åŒ–å¢å¼ºç‰ˆåœ¨çº¿æœç´¢å¼•æ“
        
        Args:
            max_search_results: æœ€å¤§æœç´¢ç»“æœæ•°é‡
            max_sub_queries: æœ€å¤§å­æŸ¥è¯¢æ•°é‡
            chunk_size: æ–‡æ¡£åˆ†å—å¤§å°
            chunk_overlap: æ–‡æ¡£åˆ†å—é‡å å¤§å°
        """
        self.max_search_results = max_search_results
        self.max_sub_queries = max_sub_queries
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap
        
        # LLMç›¸å…³ç»„ä»¶åˆå§‹åŒ–ä¸ºNoneï¼Œåœ¨ä½¿ç”¨æ—¶åŠ¨æ€è·å–
        self.llm = None
        self.embedding_model = None
        self.vector_db = None
        
        # åˆå§‹åŒ–FireCrawl
        try:
            api_key = os.getenv("FIRECRAWL_API_KEY")
            if not api_key:
                raise ValueError("FIRECRAWL_API_KEYæœªé…ç½®")
            
            self.firecrawl_app = FirecrawlApp(api_key=api_key)
            logger.info("âœ… å¢å¼ºç‰ˆåœ¨çº¿æœç´¢å¼•æ“åˆå§‹åŒ–æˆåŠŸ")
        except Exception as e:
            logger.error(f"âŒ FireCrawlåˆå§‹åŒ–å¤±è´¥: {e}")
            self.firecrawl_app = None
    
    def _ensure_components_initialized(self):
        """ç¡®ä¿ç»„ä»¶å·²æ­£ç¡®åˆå§‹åŒ–"""
        if self.llm is None or self.embedding_model is None:
            # é‡æ–°è·å–å…¨å±€ç»„ä»¶
            from deepsearcher.configuration import llm as global_llm, embedding_model as global_embedding, vector_db as global_vector_db
            
            if global_llm is None:
                logger.warning("âš ï¸ LLMæœªåˆå§‹åŒ–ï¼Œéœ€è¦å…ˆè°ƒç”¨ init_config()")
                return False
            if global_embedding is None:
                logger.warning("âš ï¸ åµŒå…¥æ¨¡å‹æœªåˆå§‹åŒ–ï¼Œéœ€è¦å…ˆè°ƒç”¨ init_config()")
                return False
            
            self.llm = global_llm
            self.embedding_model = global_embedding
            self.vector_db = global_vector_db
            logger.info("âœ… LLMç»„ä»¶å·²åŠ¨æ€åˆå§‹åŒ–")
        return True
    
    def search_and_answer(self, user_query: str) -> Tuple[str, List[Dict[str, Any]]]:
        """
        æ™ºèƒ½æœç´¢å’Œå›ç­” - ä½¿ç”¨LLMè¿›è¡Œé—®é¢˜åˆ†è§£å’Œç­”æ¡ˆç”Ÿæˆ
        
        Args:
            user_query: ç”¨æˆ·æŸ¥è¯¢
            
        Returns:
            (ç­”æ¡ˆ, ä¿¡æ¯æºåˆ—è¡¨)
        """
        try:
            logger.info(f"ğŸ§  å¼€å§‹å¢å¼ºç‰ˆåœ¨çº¿æœç´¢: {user_query}")
            
            if not self.firecrawl_app:
                return "FireCrawlæœåŠ¡æœªé…ç½®ï¼Œæ— æ³•è¿›è¡Œåœ¨çº¿æœç´¢ã€‚è¯·é…ç½®FIRECRAWL_API_KEYã€‚", []
            
            # ç¬¬ä¸€æ­¥ï¼šä½¿ç”¨LLMåˆ†è§£ç”¨æˆ·é—®é¢˜
            sub_queries = self._decompose_query_with_llm(user_query)
            logger.info(f"ğŸ” é—®é¢˜åˆ†è§£å®Œæˆï¼Œç”Ÿæˆ {len(sub_queries)} ä¸ªå­æŸ¥è¯¢")
            
            # ç¬¬äºŒæ­¥ï¼šå¯¹æ¯ä¸ªå­æŸ¥è¯¢ä½¿ç”¨FireCrawlæœç´¢
            all_documents = []
            search_results_summary = {}
            
            for i, sub_query in enumerate(sub_queries, 1):
                logger.info(f"ğŸ” æ‰§è¡Œå­æŸ¥è¯¢ {i}/{len(sub_queries)}: {sub_query}")
                documents = self._search_with_firecrawl(sub_query, user_query)
                
                if documents:
                    all_documents.extend(documents)
                    search_results_summary[sub_query] = len(documents)
                    logger.info(f"âœ… å­æŸ¥è¯¢ {i} è·å¾— {len(documents)} ä¸ªæ–‡æ¡£")
                else:
                    logger.warning(f"âš ï¸ å­æŸ¥è¯¢ {i} æœªæ‰¾åˆ°ç›¸å…³æ–‡æ¡£")
                
                # æ·»åŠ å»¶è¿Ÿé¿å…APIé¢‘ç‡é™åˆ¶
                if i < len(sub_queries):
                    time.sleep(random.uniform(1, 2))
            
            # ç¬¬ä¸‰æ­¥ï¼šå»é‡å’Œæ’åºæ–‡æ¡£
            unique_documents = self._deduplicate_and_rank_documents(all_documents, user_query)
            logger.info(f"ğŸ“š æ–‡æ¡£å¤„ç†å®Œæˆï¼Œæœ€ç»ˆè·å¾— {len(unique_documents)} ä¸ªé«˜è´¨é‡æ–‡æ¡£")
            
            if not unique_documents:
                return "æŠ±æ­‰ï¼Œæ²¡æœ‰æ‰¾åˆ°ç›¸å…³çš„åä¸ºæŠ€æœ¯æ–‡æ¡£ã€‚å»ºè®®å°è¯•æ›´å…·ä½“çš„å…³é”®è¯æˆ–æŸ¥çœ‹åä¸ºå¼€å‘è€…å®˜ç½‘ã€‚", []
            
            # ç¬¬å››æ­¥ï¼šä½¿ç”¨LLMåŸºäºæœç´¢ç»“æœç”Ÿæˆç»¼åˆç­”æ¡ˆ
            answer = self._generate_comprehensive_answer_with_llm(
                user_query, unique_documents, sub_queries, search_results_summary
            )
            
            # ç¬¬äº”æ­¥ï¼šå‡†å¤‡ä¿¡æ¯æº
            sources = self._prepare_sources_info(unique_documents)
            
            logger.info(f"ğŸ‰ å¢å¼ºç‰ˆæœç´¢å®Œæˆï¼Œç”Ÿæˆç­”æ¡ˆé•¿åº¦: {len(answer)} å­—ç¬¦ï¼Œä¿¡æ¯æº: {len(sources)} ä¸ª")
            return answer, sources
            
        except Exception as e:
            logger.error(f"âŒ å¢å¼ºç‰ˆåœ¨çº¿æœç´¢å¤±è´¥: {e}")
            return f"æœç´¢è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}", []
    
    def hybrid_search_and_answer(self, 
                                user_query: str, 
                                use_local: bool = True,
                                use_online: bool = True,
                                collection_name: str = None) -> Dict[str, Any]:
        """
        æ··åˆæœç´¢ï¼šç»“åˆæœ¬åœ°æ•°æ®åº“å’Œåœ¨çº¿æœç´¢
        
        Args:
            user_query: ç”¨æˆ·æŸ¥è¯¢
            use_local: æ˜¯å¦ä½¿ç”¨æœ¬åœ°æœç´¢
            use_online: æ˜¯å¦ä½¿ç”¨åœ¨çº¿æœç´¢
            collection_name: æœ¬åœ°æœç´¢çš„é›†åˆåç§°
            
        Returns:
            åŒ…å«ç»¼åˆç­”æ¡ˆå’Œæ¥æºä¿¡æ¯çš„å­—å…¸
        """
        logger.info(f"ğŸ”€ å¼€å§‹æ··åˆæœç´¢: {user_query}")
        
        local_results = []
        online_answer = ""
        online_sources = []
        
        # æœ¬åœ°æœç´¢
        if use_local:
            try:
                from .adapter import HuaweiDeepSearcherAdapter
                adapter = HuaweiDeepSearcherAdapter()
                local_results = adapter.search_huawei_docs(user_query, top_k=5)
                logger.info(f"ğŸ“š æœ¬åœ°æœç´¢å®Œæˆï¼Œæ‰¾åˆ° {len(local_results)} ä¸ªç»“æœ")
            except Exception as e:
                logger.warning(f"âš ï¸ æœ¬åœ°æœç´¢å¤±è´¥: {e}")
        
        # åœ¨çº¿æœç´¢
        if use_online:
            try:
                online_answer, online_sources = self.search_and_answer(user_query)
                logger.info(f"ğŸŒ åœ¨çº¿æœç´¢å®Œæˆï¼Œç”Ÿæˆç­”æ¡ˆé•¿åº¦: {len(online_answer)} å­—ç¬¦")
            except Exception as e:
                logger.warning(f"âš ï¸ åœ¨çº¿æœç´¢å¤±è´¥: {e}")
        
        # ç”Ÿæˆæ··åˆç­”æ¡ˆ
        if local_results or online_answer:
            final_answer = self._generate_hybrid_answer(
                user_query, local_results, online_answer, online_sources
            )
            
            # å‡†å¤‡æ¥æºä¿¡æ¯
            sources = []
            
            # æ·»åŠ æœ¬åœ°æ¥æº
            for result in local_results:
                sources.append({
                    'type': 'local',
                    'title': result.get('title', 'æœ¬åœ°æ–‡æ¡£'),
                    'url': result.get('url', ''),
                    'relevance_score': result.get('score', 0)
                })
            
            # æ·»åŠ åœ¨çº¿æ¥æº
            for source in online_sources:
                source['type'] = 'online'
                sources.append(source)
            
            return {
                'final_answer': final_answer,
                'sources': sources,
                'local_results_count': len(local_results),
                'online_sources_count': len(online_sources)
            }
        else:
            return {
                'final_answer': "æŠ±æ­‰ï¼Œæ²¡æœ‰æ‰¾åˆ°ç›¸å…³ä¿¡æ¯ã€‚å»ºè®®å°è¯•æ›´å…·ä½“çš„å…³é”®è¯ã€‚",
                'sources': [],
                'local_results_count': 0,
                'online_sources_count': 0
            }
    
    def _decompose_query_with_llm(self, user_query: str) -> List[str]:
        """
        ä½¿ç”¨LLMå°†ç”¨æˆ·æŸ¥è¯¢åˆ†è§£ä¸ºå¤šä¸ªå­æŸ¥è¯¢
        
        Args:
            user_query: ç”¨æˆ·åŸå§‹æŸ¥è¯¢
            
        Returns:
            åˆ†è§£åçš„å­æŸ¥è¯¢åˆ—è¡¨
        """
        try:
            if not self._ensure_components_initialized():
                logger.warning("âš ï¸ LLMæœªåˆå§‹åŒ–ï¼Œä½¿ç”¨é»˜è®¤æŸ¥è¯¢åˆ†è§£ç­–ç•¥")
                return self._fallback_query_decomposition(user_query)
            
            # æ„å»ºé—®é¢˜åˆ†è§£çš„æç¤ºè¯
            decomposition_prompt = f"""
ä½œä¸ºåä¸ºæŠ€æœ¯æ–‡æ¡£æœç´¢ä¸“å®¶ï¼Œè¯·å°†ç”¨æˆ·çš„æŸ¥è¯¢åˆ†è§£ä¸ºå¤šä¸ªå…·ä½“çš„å­æŸ¥è¯¢ï¼Œä»¥ä¾¿æ›´å…¨é¢åœ°æœç´¢ç›¸å…³ä¿¡æ¯ã€‚

ç”¨æˆ·æŸ¥è¯¢ï¼š{user_query}

è¯·éµå¾ªä»¥ä¸‹åŸåˆ™ï¼š
1. åˆ†è§£ä¸º3-5ä¸ªå…·ä½“çš„å­æŸ¥è¯¢
2. æ¯ä¸ªå­æŸ¥è¯¢åº”è¯¥èšç„¦äºé—®é¢˜çš„ä¸€ä¸ªç‰¹å®šæ–¹é¢
3. ä¼˜å…ˆè€ƒè™‘åä¸ºæŠ€æœ¯æ ˆç›¸å…³çš„æŸ¥è¯¢
4. åŒ…å«ä¸åŒå±‚æ¬¡çš„æŸ¥è¯¢ï¼ˆæ¦‚å¿µã€å®ç°ã€ç¤ºä¾‹ã€æœ€ä½³å®è·µç­‰ï¼‰
5. ç¡®ä¿æŸ¥è¯¢é€‚åˆåœ¨åä¸ºå¼€å‘è€…æ–‡æ¡£ä¸­æœç´¢

è¯·ç›´æ¥è¾“å‡ºå­æŸ¥è¯¢åˆ—è¡¨ï¼Œæ¯è¡Œä¸€ä¸ªï¼Œä¸è¦æ·»åŠ ç¼–å·æˆ–å…¶ä»–æ ¼å¼ï¼š
"""
            
            # è°ƒç”¨LLMè¿›è¡Œé—®é¢˜åˆ†è§£
            logger.info("ğŸ§  æ­£åœ¨ä½¿ç”¨LLMåˆ†è§£æŸ¥è¯¢...")
            messages = [{"role": "user", "content": decomposition_prompt}]
            response = self.llm.chat(messages)
            
            # è§£æLLMå“åº”
            sub_queries = []
            for line in response.content.strip().split('\n'):
                line = line.strip()
                if line and not line.startswith(('#', '-', '*', '1.', '2.', '3.', '4.', '5.')):
                    # æ¸…ç†å¯èƒ½çš„ç¼–å·å‰ç¼€
                    cleaned_line = line
                    for prefix in ['1.', '2.', '3.', '4.', '5.', '-', '*', 'â€¢']:
                        if cleaned_line.startswith(prefix):
                            cleaned_line = cleaned_line[len(prefix):].strip()
                    
                    if cleaned_line:
                        sub_queries.append(cleaned_line)
            
            # é™åˆ¶å­æŸ¥è¯¢æ•°é‡å¹¶æ·»åŠ åŸå§‹æŸ¥è¯¢
            sub_queries = sub_queries[:self.max_sub_queries-1]
            if user_query not in sub_queries:
                sub_queries.insert(0, user_query)  # ç¡®ä¿åŸå§‹æŸ¥è¯¢åœ¨ç¬¬ä¸€ä½
            
            logger.info(f"âœ… LLMåˆ†è§£æŸ¥è¯¢æˆåŠŸï¼Œç”Ÿæˆ {len(sub_queries)} ä¸ªå­æŸ¥è¯¢")
            return sub_queries
            
        except Exception as e:
            logger.warning(f"âš ï¸ LLMæŸ¥è¯¢åˆ†è§£å¤±è´¥: {e}ï¼Œä½¿ç”¨å¤‡ç”¨ç­–ç•¥")
            return self._fallback_query_decomposition(user_query)
    
    def _fallback_query_decomposition(self, user_query: str) -> List[str]:
        """
        å¤‡ç”¨æŸ¥è¯¢åˆ†è§£ç­–ç•¥ï¼ˆå½“LLMä¸å¯ç”¨æ—¶ï¼‰
        
        Args:
            user_query: ç”¨æˆ·åŸå§‹æŸ¥è¯¢
            
        Returns:
            åˆ†è§£åçš„å­æŸ¥è¯¢åˆ—è¡¨
        """
        # åŸºäºå…³é”®è¯çš„ç®€å•åˆ†è§£ç­–ç•¥
        base_queries = [user_query]
        
        # æ·»åŠ åä¸ºç›¸å…³çš„æ‰©å±•æŸ¥è¯¢
        huawei_terms = ["åä¸º", "Huawei", "HMS", "HarmonyOS", "é¸¿è’™"]
        for term in huawei_terms:
            if term.lower() not in user_query.lower():
                base_queries.append(f"{user_query} {term}")
                break
        
        # æ·»åŠ æŠ€æœ¯ç›¸å…³çš„æ‰©å±•æŸ¥è¯¢
        if "å¼€å‘" not in user_query and "API" not in user_query:
            base_queries.append(f"{user_query} å¼€å‘æŒ‡å—")
        
        if "ç¤ºä¾‹" not in user_query and "ä¾‹å­" not in user_query:
            base_queries.append(f"{user_query} ç¤ºä¾‹")
        
        return base_queries[:self.max_sub_queries]
    
    def _search_with_firecrawl(self, query: str, original_query: str) -> List[Document]:
        """
        ä½¿ç”¨FireCrawlè¿›è¡Œæœç´¢
        
        Args:
            query: æœç´¢æŸ¥è¯¢
            original_query: åŸå§‹ç”¨æˆ·æŸ¥è¯¢
            
        Returns:
            æœç´¢åˆ°çš„æ–‡æ¡£åˆ—è¡¨
        """
        try:
            if not self.firecrawl_app:
                logger.warning("âš ï¸ FireCrawlæœªåˆå§‹åŒ–")
                return []
            
            # ç”Ÿæˆåä¸ºä¼˜åŒ–çš„æœç´¢æŸ¥è¯¢
            optimized_queries = self._generate_huawei_optimized_queries(query)
            
            documents = []
            for search_query in optimized_queries:
                try:
                    logger.info(f"ğŸ”¥ FireCrawlæœç´¢: {search_query}")
                    
                    # ä½¿ç”¨FireCrawlçš„æœç´¢åŠŸèƒ½ - é€šè¿‡REST APIè°ƒç”¨
                    import requests
                    import os
                    
                    api_key = os.getenv('FIRECRAWL_API_KEY')
                    if not api_key:
                        logger.error("âŒ FireCrawl APIå¯†é’¥æœªé…ç½®")
                        continue
                    
                    headers = {
                        'Content-Type': 'application/json',
                        'Authorization': f'Bearer {api_key}'
                    }
                    
                    payload = {
                        'query': search_query,
                        'limit': 5,
                        'scrapeOptions': {
                            'formats': ['markdown', 'html']
                        }
                    }
                    
                    response = requests.post(
                        'https://api.firecrawl.dev/v1/search',
                        json=payload,
                        headers=headers,
                        timeout=30
                    )
                    
                    if response.status_code == 200:
                        search_result = response.json()
                        
                        if search_result.get('success') and search_result.get('data'):
                            for item in search_result['data']:
                                doc = self._process_firecrawl_search_result(item, query, original_query)
                                if doc:
                                    documents.append(doc)
                            
                            logger.info(f"âœ… FireCrawlæœç´¢ '{search_query}' è¿”å› {len(search_result['data'])} ä¸ªç»“æœ")
                        else:
                            logger.warning(f"âš ï¸ FireCrawlæœç´¢ '{search_query}' æ— ç»“æœ")
                    else:
                        logger.error(f"âŒ FireCrawlæœç´¢å¤±è´¥ '{search_query}': HTTP {response.status_code}")
                    
                    # æ·»åŠ å»¶è¿Ÿé¿å…APIé¢‘ç‡é™åˆ¶
                    time.sleep(random.uniform(0.5, 1.0))
                    
                except Exception as e:
                    if "429" in str(e) or "rate limit" in str(e).lower():
                        logger.warning(f"âš ï¸ FireCrawl APIé¢‘ç‡é™åˆ¶ï¼Œè·³è¿‡æŸ¥è¯¢: {search_query}")
                        time.sleep(2)
                        continue
                    else:
                        logger.error(f"âŒ FireCrawlæœç´¢å¤±è´¥ '{search_query}': {e}")
                        continue
            
            return documents
            
        except Exception as e:
            logger.error(f"âŒ FireCrawlæœç´¢æ•´ä½“å¤±è´¥: {e}")
            return []
    
    def _generate_huawei_optimized_queries(self, user_query: str) -> List[str]:
        """
        ç”Ÿæˆåä¸ºä¼˜åŒ–çš„æœç´¢æŸ¥è¯¢
        
        Args:
            user_query: ç”¨æˆ·æŸ¥è¯¢
            
        Returns:
            ä¼˜åŒ–åçš„æŸ¥è¯¢åˆ—è¡¨
        """
        queries = []
        
        # åŸå§‹æŸ¥è¯¢
        queries.append(user_query)
        
        # æ·»åŠ åä¸ºåŸŸåé™åˆ¶çš„æŸ¥è¯¢
        huawei_domains = [
            "site:developer.huawei.com",
            "site:developer.harmonyos.com", 
            "site:consumer.huawei.com",
            "site:forums.developer.huawei.com"
        ]
        
        for domain in huawei_domains:
            queries.append(f"{user_query} {domain}")
        
        return queries[:3]  # é™åˆ¶æŸ¥è¯¢æ•°é‡é¿å…è¿‡åº¦è°ƒç”¨API
    
    def _process_firecrawl_search_result(self, item: Dict, query: str, user_query: str) -> Optional[Document]:
        """
        å¤„ç†FireCrawlæœç´¢ç»“æœ
        
        Args:
            item: FireCrawlæœç´¢ç»“æœé¡¹
            query: æœç´¢æŸ¥è¯¢
            user_query: åŸå§‹ç”¨æˆ·æŸ¥è¯¢
            
        Returns:
            å¤„ç†åçš„æ–‡æ¡£å¯¹è±¡
        """
        try:
            url = item.get('url', '')
            title = item.get('title', '')
            # ä¼˜å…ˆä½¿ç”¨markdownå†…å®¹ï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨description
            content = item.get('markdown', '') or item.get('content', '') or item.get('description', '')
            description = item.get('description', '')
            
            if not content and not description:
                return None
            
            # æ£€æŸ¥æ˜¯å¦ä¸ºåä¸ºå®˜æ–¹å†…å®¹
            is_huawei_official = self._is_huawei_official_content(url, title, content)
            
            # è®¡ç®—ç›¸å…³æ€§åˆ†æ•°
            relevance_score = self._calculate_relevance_score(content, title, description, user_query)
            
            # åˆ›å»ºæ–‡æ¡£å¯¹è±¡
            full_content = f"{title}\n\n{content}"
            if description and description not in content:
                full_content += f"\n\n{description}"
            
            metadata = {
                'source': url,
                'title': title,
                'description': description,
                'query': query,
                'user_query': user_query,
                'relevance_score': relevance_score,
                'is_huawei_official': is_huawei_official,
                'timestamp': time.time()
            }
            
            return Document(page_content=full_content, metadata=metadata)
            
        except Exception as e:
            logger.error(f"âŒ å¤„ç†FireCrawlæœç´¢ç»“æœå¤±è´¥: {e}")
            return None
    
    def _is_huawei_official_content(self, url: str, title: str, content: str) -> bool:
        """
        åˆ¤æ–­æ˜¯å¦ä¸ºåä¸ºå®˜æ–¹å†…å®¹
        
        Args:
            url: é¡µé¢URL
            title: é¡µé¢æ ‡é¢˜
            content: é¡µé¢å†…å®¹
            
        Returns:
            æ˜¯å¦ä¸ºåä¸ºå®˜æ–¹å†…å®¹
        """
        huawei_domains = [
            'developer.huawei.com',
            'developer.harmonyos.com',
            'consumer.huawei.com',
            'forums.developer.huawei.com',
            'huaweicloud.com'
        ]
        
        # æ£€æŸ¥URLåŸŸå
        for domain in huawei_domains:
            if domain in url.lower():
                return True
        
        # æ£€æŸ¥æ ‡é¢˜å’Œå†…å®¹ä¸­çš„åä¸ºæ ‡è¯†
        huawei_indicators = ['åä¸º', 'huawei', 'hms', 'harmonyos', 'é¸¿è’™']
        text_to_check = f"{title} {content}".lower()
        
        return any(indicator in text_to_check for indicator in huawei_indicators)
    
    def _calculate_relevance_score(self, content: str, title: str, description: str, user_query: str) -> float:
        """
        è®¡ç®—å†…å®¹ç›¸å…³æ€§åˆ†æ•°
        
        Args:
            content: é¡µé¢å†…å®¹
            title: é¡µé¢æ ‡é¢˜
            description: é¡µé¢æè¿°
            user_query: ç”¨æˆ·æŸ¥è¯¢
            
        Returns:
            ç›¸å…³æ€§åˆ†æ•° (0-1)
        """
        score = 0.0
        query_terms = user_query.lower().split()
        
        # æ£€æŸ¥æ ‡é¢˜åŒ¹é…
        title_lower = title.lower()
        title_matches = sum(1 for term in query_terms if term in title_lower)
        score += (title_matches / len(query_terms)) * 0.4
        
        # æ£€æŸ¥å†…å®¹åŒ¹é…
        content_lower = content.lower()
        content_matches = sum(1 for term in query_terms if term in content_lower)
        score += (content_matches / len(query_terms)) * 0.4
        
        # æ£€æŸ¥æè¿°åŒ¹é…
        if description:
            desc_lower = description.lower()
            desc_matches = sum(1 for term in query_terms if term in desc_lower)
            score += (desc_matches / len(query_terms)) * 0.2
        
        return min(score, 1.0)
    
    def _deduplicate_and_rank_documents(self, documents: List[Document], user_query: str) -> List[Document]:
        """
        å»é‡å’Œæ’åºæ–‡æ¡£
        
        Args:
            documents: æ–‡æ¡£åˆ—è¡¨
            user_query: ç”¨æˆ·æŸ¥è¯¢
            
        Returns:
            å»é‡æ’åºåçš„æ–‡æ¡£åˆ—è¡¨
        """
        if not documents:
            return []
        
        # åŸºäºURLå»é‡
        seen_urls = set()
        unique_docs = []
        
        for doc in documents:
            url = doc.metadata.get('source', '')
            if url not in seen_urls:
                seen_urls.add(url)
                unique_docs.append(doc)
        
        # æ’åºï¼šåä¸ºå®˜æ–¹å†…å®¹ä¼˜å…ˆï¼Œç„¶åæŒ‰ç›¸å…³æ€§åˆ†æ•°æ’åº
        def sort_key(doc):
            is_official = doc.metadata.get('is_huawei_official', False)
            relevance = doc.metadata.get('relevance_score', 0)
            return (not is_official, -relevance)  # å®˜æ–¹å†…å®¹ä¼˜å…ˆï¼Œç›¸å…³æ€§é«˜çš„ä¼˜å…ˆ
        
        unique_docs.sort(key=sort_key)
        
        # é™åˆ¶è¿”å›æ•°é‡
        return unique_docs[:self.max_search_results]
    
    def _generate_comprehensive_answer_with_llm(self, 
                                               user_query: str, 
                                               documents: List[Document], 
                                               sub_queries: List[str],
                                               search_results_summary: Dict[str, int]) -> str:
        """
        ä½¿ç”¨LLMç”Ÿæˆç»¼åˆç­”æ¡ˆ
        
        Args:
            user_query: ç”¨æˆ·æŸ¥è¯¢
            documents: æœç´¢åˆ°çš„æ–‡æ¡£
            sub_queries: å­æŸ¥è¯¢åˆ—è¡¨
            search_results_summary: æœç´¢ç»“æœæ‘˜è¦
            
        Returns:
            ç”Ÿæˆçš„ç»¼åˆç­”æ¡ˆ
        """
        try:
            if not self._ensure_components_initialized():
                logger.warning("âš ï¸ LLMæœªåˆå§‹åŒ–ï¼Œä½¿ç”¨ç®€å•ç­”æ¡ˆç”Ÿæˆ")
                return self._generate_simple_answer(user_query, documents)
            
            # å‡†å¤‡æ–‡æ¡£å†…å®¹
            context_parts = []
            for i, doc in enumerate(documents, 1):
                title = doc.metadata.get('title', f'æ–‡æ¡£{i}')
                url = doc.metadata.get('source', '')
                content = doc.page_content[:1500]  # é™åˆ¶é•¿åº¦é¿å…tokenè¿‡å¤š
                
                context_part = f"## æ¥æº{i}: {title}\n"
                if url:
                    context_part += f"é“¾æ¥: {url}\n"
                context_part += f"å†…å®¹: {content}\n"
                context_parts.append(context_part)
            
            context = "\n".join(context_parts)
            
            # æ„å»ºç»¼åˆç­”æ¡ˆç”Ÿæˆçš„æç¤ºè¯
            answer_prompt = f"""
ä½œä¸ºåä¸ºæŠ€æœ¯ä¸“å®¶ï¼Œè¯·åŸºäºä»¥ä¸‹æœç´¢åˆ°çš„åä¸ºå®˜æ–¹æ–‡æ¡£å’ŒæŠ€æœ¯èµ„æ–™ï¼Œä¸ºç”¨æˆ·æä¾›å‡†ç¡®ã€å…¨é¢çš„ç­”æ¡ˆã€‚

ç”¨æˆ·é—®é¢˜: {user_query}

æœç´¢ç­–ç•¥: 
- æ‰§è¡Œäº† {len(sub_queries)} ä¸ªç›¸å…³æŸ¥è¯¢
- å…±æ‰¾åˆ° {len(documents)} ä¸ªç›¸å…³æ–‡æ¡£
- æŸ¥è¯¢åˆ†è§£: {', '.join(sub_queries)}

ç›¸å…³æ–‡æ¡£å†…å®¹:
{context}

è¯·éµå¾ªä»¥ä¸‹è¦æ±‚:
1. åŸºäºæä¾›çš„åä¸ºå®˜æ–¹æ–‡æ¡£å†…å®¹å›ç­”é—®é¢˜
2. ç­”æ¡ˆè¦å‡†ç¡®ã€è¯¦ç»†ã€å®ç”¨
3. å¦‚æœæ¶‰åŠä»£ç ç¤ºä¾‹ï¼Œè¯·æä¾›å…·ä½“çš„å®ç°æ–¹æ³•
4. çªå‡ºåä¸ºæŠ€æœ¯æ ˆçš„ç‰¹ç‚¹å’Œä¼˜åŠ¿
5. å¦‚æœæœ‰å¤šä¸ªç›¸å…³æ–¹é¢ï¼Œè¯·åˆ†ç‚¹è¯¦ç»†è¯´æ˜
6. åœ¨ç­”æ¡ˆæœ«å°¾ç®€è¦è¯´æ˜ä¿¡æ¯æ¥æºçš„å¯é æ€§

è¯·ç”Ÿæˆä¸€ä¸ªä¸“ä¸šã€è¯¦ç»†çš„ç­”æ¡ˆ:
"""
            
            # è°ƒç”¨LLMç”Ÿæˆç­”æ¡ˆ
            logger.info("ğŸ§  æ­£åœ¨ä½¿ç”¨LLMç”Ÿæˆç»¼åˆç­”æ¡ˆ...")
            messages = [{"role": "user", "content": answer_prompt}]
            response = self.llm.chat(messages)
            
            answer = response.content.strip()
            logger.info(f"âœ… LLMç­”æ¡ˆç”ŸæˆæˆåŠŸï¼Œé•¿åº¦: {len(answer)} å­—ç¬¦")
            
            return answer
            
        except Exception as e:
            logger.warning(f"âš ï¸ LLMç­”æ¡ˆç”Ÿæˆå¤±è´¥: {e}ï¼Œä½¿ç”¨ç®€å•ç­”æ¡ˆç”Ÿæˆ")
            return self._generate_simple_answer(user_query, documents)
    
    def _generate_simple_answer(self, user_query: str, documents: List[Document]) -> str:
        """
        ç”Ÿæˆç®€å•ç­”æ¡ˆï¼ˆå½“LLMä¸å¯ç”¨æ—¶ï¼‰
        
        Args:
            user_query: ç”¨æˆ·æŸ¥è¯¢
            documents: æœç´¢åˆ°çš„æ–‡æ¡£
            
        Returns:
            ç®€å•çš„ç­”æ¡ˆ
        """
        if not documents:
            return "æŠ±æ­‰ï¼Œæ²¡æœ‰æ‰¾åˆ°ç›¸å…³çš„åä¸ºæŠ€æœ¯æ–‡æ¡£ã€‚"
        
        answer_parts = [f"æ ¹æ®æœç´¢åˆ°çš„ {len(documents)} ä¸ªåä¸ºæŠ€æœ¯æ–‡æ¡£ï¼Œä»¥ä¸‹æ˜¯ç›¸å…³ä¿¡æ¯ï¼š\n"]
        
        for i, doc in enumerate(documents, 1):
            title = doc.metadata.get('title', f'æ–‡æ¡£{i}')
            url = doc.metadata.get('source', '')
            content = doc.page_content[:300]  # é™åˆ¶é•¿åº¦
            
            answer_parts.append(f"## {i}. {title}")
            if url:
                answer_parts.append(f"æ¥æº: {url}")
            answer_parts.append(f"{content}...\n")
        
        answer_parts.append("\nä»¥ä¸Šä¿¡æ¯æ¥è‡ªåä¸ºå®˜æ–¹æ–‡æ¡£å’ŒæŠ€æœ¯èµ„æ–™ï¼Œå»ºè®®æŸ¥çœ‹åŸå§‹é“¾æ¥è·å–å®Œæ•´ä¿¡æ¯ã€‚")
        
        return "\n".join(answer_parts)
    
    def _generate_hybrid_answer(self, 
                              user_query: str, 
                              local_results: List[Dict], 
                              online_answer: str,
                              online_sources: List[Dict]) -> str:
        """
        ç”Ÿæˆæ··åˆæœç´¢çš„ç»¼åˆç­”æ¡ˆ
        
        Args:
            user_query: ç”¨æˆ·æŸ¥è¯¢
            local_results: æœ¬åœ°æœç´¢ç»“æœ
            online_answer: åœ¨çº¿æœç´¢ç­”æ¡ˆ
            online_sources: åœ¨çº¿ä¿¡æ¯æº
            
        Returns:
            ç»¼åˆç­”æ¡ˆ
        """
        try:
            if not self._ensure_components_initialized():
                # ç®€å•æ‹¼æ¥ç­”æ¡ˆ
                answer_parts = []
                
                if local_results:
                    answer_parts.append("## æœ¬åœ°çŸ¥è¯†åº“ä¿¡æ¯:")
                    for i, result in enumerate(local_results[:3], 1):
                        title = result.get('title', f'æ–‡æ¡£{i}')
                        content = result.get('content', '')[:200]
                        answer_parts.append(f"{i}. {title}: {content}...")
                
                if online_answer:
                    answer_parts.append("\n## åœ¨çº¿æœç´¢ä¿¡æ¯:")
                    answer_parts.append(online_answer)
                
                return "\n".join(answer_parts)
            
            # ä½¿ç”¨LLMç”Ÿæˆç»¼åˆç­”æ¡ˆ
            local_context = ""
            if local_results:
                local_parts = []
                for result in local_results[:3]:
                    title = result.get('title', 'æœ¬åœ°æ–‡æ¡£')
                    content = result.get('content', '')[:500]
                    local_parts.append(f"- {title}: {content}")
                local_context = "\n".join(local_parts)
            
            hybrid_prompt = f"""
ä½œä¸ºåä¸ºæŠ€æœ¯ä¸“å®¶ï¼Œè¯·åŸºäºæœ¬åœ°çŸ¥è¯†åº“å’Œåœ¨çº¿æœç´¢çš„ä¿¡æ¯ï¼Œä¸ºç”¨æˆ·æä¾›ç»¼åˆæ€§çš„ç­”æ¡ˆã€‚

ç”¨æˆ·é—®é¢˜: {user_query}

æœ¬åœ°çŸ¥è¯†åº“ä¿¡æ¯:
{local_context if local_context else "æ— ç›¸å…³æœ¬åœ°ä¿¡æ¯"}

åœ¨çº¿æœç´¢ä¿¡æ¯:
{online_answer if online_answer else "æ— åœ¨çº¿æœç´¢ç»“æœ"}

è¯·ç»¼åˆä»¥ä¸Šä¿¡æ¯ï¼Œç”Ÿæˆä¸€ä¸ªå‡†ç¡®ã€å…¨é¢çš„ç­”æ¡ˆï¼š
1. æ•´åˆæœ¬åœ°å’Œåœ¨çº¿ä¿¡æ¯
2. å»é™¤é‡å¤å†…å®¹
3. çªå‡ºæœ€é‡è¦çš„ä¿¡æ¯
4. ä¿æŒé€»è¾‘æ¸…æ™°
"""
            
            messages = [{"role": "user", "content": hybrid_prompt}]
            response = self.llm.chat(messages)
            
            return response.content.strip()
            
        except Exception as e:
            logger.warning(f"âš ï¸ æ··åˆç­”æ¡ˆç”Ÿæˆå¤±è´¥: {e}")
            # è¿”å›ç®€å•æ‹¼æ¥çš„ç­”æ¡ˆ
            answer_parts = []
            
            if local_results:
                answer_parts.append("## æœ¬åœ°çŸ¥è¯†åº“ä¿¡æ¯:")
                for i, result in enumerate(local_results[:3], 1):
                    title = result.get('title', f'æ–‡æ¡£{i}')
                    content = result.get('content', '')[:200]
                    answer_parts.append(f"{i}. {title}: {content}...")
            
            if online_answer:
                answer_parts.append("\n## åœ¨çº¿æœç´¢ä¿¡æ¯:")
                answer_parts.append(online_answer)
            
            return "\n".join(answer_parts) if answer_parts else "æœªæ‰¾åˆ°ç›¸å…³ä¿¡æ¯ã€‚"
    
    def _prepare_sources_info(self, documents: List[Document]) -> List[Dict[str, Any]]:
        """
        å‡†å¤‡ä¿¡æ¯æºä¿¡æ¯
        
        Args:
            documents: æ–‡æ¡£åˆ—è¡¨
            
        Returns:
            ä¿¡æ¯æºåˆ—è¡¨
        """
        sources = []
        for doc in documents:
            source_info = {
                'title': doc.metadata.get('title', 'æœªçŸ¥æ ‡é¢˜'),
                'url': doc.metadata.get('source', ''),
                'description': doc.metadata.get('description', ''),
                'relevance_score': doc.metadata.get('relevance_score', 0),
                'is_huawei_official': doc.metadata.get('is_huawei_official', False)
            }
            sources.append(source_info)
        
        return sources 