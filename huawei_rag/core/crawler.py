#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
åä¸ºå¼€å‘è€…æ–‡æ¡£å†…å®¹çˆ¬è™« - é‡æ„ç‰ˆ
ç²¾ç®€äº†åŸå§‹çˆ¬è™«çš„åŠŸèƒ½ï¼Œä¿ç•™æ ¸å¿ƒçš„å†…å®¹æå–å’Œè¾¹çˆ¬è¾¹å­˜åŠŸèƒ½
"""

import asyncio
import json
import os
import time
import logging
from datetime import datetime
from typing import Dict, List, Optional, Any
from dataclasses import dataclass, asdict
from playwright.async_api import async_playwright, Page, Browser

from .config import CrawlerConfig, PAGE_TYPE_CONFIGS

# é…ç½®æ—¥å¿—
logger = logging.getLogger(__name__)

@dataclass
class PageContent:
    """é¡µé¢å†…å®¹æ•°æ®ç»“æ„"""
    url: str
    title: str
    text_content: str = ""
    code_blocks: List[Dict[str, str]] = None
    metadata: Dict[str, Any] = None
    crawl_time: str = ""
    page_type: str = "unknown"
    content_length: int = 0
    
    def __post_init__(self):
        if self.code_blocks is None:
            self.code_blocks = []
        if self.metadata is None:
            self.metadata = {}
        if not self.crawl_time:
            self.crawl_time = datetime.now().isoformat()
        self.content_length = len(self.text_content)


class ContentExtractionResult:
    """å†…å®¹æå–ç»“æœ"""
    def __init__(self, content: Optional[PageContent] = None, 
                 should_retry: bool = True, 
                 reason: str = ""):
        self.content = content
        self.should_retry = should_retry
        self.reason = reason
    
    @property
    def success(self) -> bool:
        return self.content is not None
    
    @classmethod
    def success_result(cls, content: PageContent):
        return cls(content=content, should_retry=False, reason="success")
    
    @classmethod
    def insufficient_content(cls, url: str, details: str = ""):
        return cls(content=None, should_retry=False, 
                  reason=f"å†…å®¹ä¸è¶³ï¼Œè·³è¿‡: {details}")
    
    @classmethod
    def technical_error(cls, error: str):
        return cls(content=None, should_retry=True, reason=f"æŠ€æœ¯é”™è¯¯: {error}")


class HuaweiContentCrawler:
    """åä¸ºå¼€å‘è€…æ–‡æ¡£å†…å®¹çˆ¬è™« - é‡æ„ç‰ˆ"""
    
    def __init__(self, links_file: str = None, config: CrawlerConfig = None):
        self.config = config or CrawlerConfig()
        self.links_file = links_file or self.config.LINKS_FILE
        self.crawled_content: Dict[str, PageContent] = {}
        self.failed_urls: List[Dict] = []
        self.skipped_urls: List[Dict] = []
        
        # è¾¹çˆ¬è¾¹å­˜é…ç½®
        self.save_interval = self.config.INCREMENTAL_SAVE_INTERVAL
        self.enable_incremental_save = self.config.ENABLE_INCREMENTAL_SAVE
        self.crawled_count = 0
        self.last_save_count = 0
        self._save_lock = asyncio.Lock()
        
        # ç¡®ä¿ç›®å½•å­˜åœ¨
        self.config.ensure_directories()
    
    def load_links(self) -> List[Dict]:
        """ä»JSONæ–‡ä»¶åŠ è½½é“¾æ¥"""
        try:
            with open(self.links_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            urls = []
            for url, info in data.items():
                urls.append({
                    'url': url,
                    'title': info.get('title', ''),
                    'level': info.get('level', 0)
                })
                self._extract_child_urls(info.get('children', []), urls)
            
            logger.info(f"ä» {self.links_file} åŠ è½½äº† {len(urls)} ä¸ªé“¾æ¥")
            return urls
            
        except Exception as e:
            logger.error(f"åŠ è½½é“¾æ¥æ–‡ä»¶å¤±è´¥: {e}")
            return []
    
    def _extract_child_urls(self, children: List[Dict], urls: List[Dict]):
        """é€’å½’æå–å­é¡µé¢URL"""
        for child in children:
            urls.append({
                'url': child.get('url', ''),
                'title': child.get('title', ''),
                'level': child.get('level', 0)
            })
            if child.get('children'):
                self._extract_child_urls(child['children'], urls)
    
    def _detect_page_type(self, url: str) -> str:
        """æ£€æµ‹é¡µé¢ç±»å‹"""
        url_lower = url.lower()
        if 'api' in url_lower or 'reference' in url_lower:
            return 'api_docs'
        elif 'tutorial' in url_lower or 'guide' in url_lower:
            return 'tutorial'
        return 'unknown'
    
    def _get_page_config(self, page_type: str, url: str) -> Dict:
        """è·å–é¡µé¢ç‰¹å®šé…ç½®"""
        config = {
            'selectors': self.config.CONTENT_SELECTORS,
            'code_selectors': self.config.CODE_SELECTORS,
            'wait_time': self.config.WAIT_FOR_CONTENT_TIMEOUT
        }
        
        if page_type in PAGE_TYPE_CONFIGS:
            type_config = PAGE_TYPE_CONFIGS[page_type]
            config['selectors'] = type_config['selectors'] + config['selectors']
            config['code_selectors'] = type_config['code_selectors'] + config['code_selectors']
            config['wait_time'] = type_config['wait_time']
        
        return config
    
    async def setup_page(self, browser: Browser) -> Page:
        """è®¾ç½®é¡µé¢"""
        context = await browser.new_context(
            user_agent=self.config.USER_AGENT,
            viewport=self.config.VIEWPORT
        )
        
        page = await context.new_page()
        return page
    
    async def extract_content(self, page: Page, url: str) -> ContentExtractionResult:
        """æå–é¡µé¢å†…å®¹"""
        page_type = self._detect_page_type(url)
        page_config = self._get_page_config(page_type, url)
        
        try:
            response = await page.goto(
                url, 
                wait_until='domcontentloaded', 
                timeout=self.config.REQUEST_TIMEOUT
            )
            
            if not response or response.status != 200:
                if self.config.FILTER_CONFIG['skip_error_pages']:
                    return ContentExtractionResult.insufficient_content(
                        url, f"HTTPçŠ¶æ€ç : {response.status if response else 'None'}")
                else:
                    return ContentExtractionResult.technical_error(
                        f"HTTPé”™è¯¯ï¼ŒçŠ¶æ€ç : {response.status if response else 'None'}")
            
            await page.wait_for_load_state('networkidle', timeout=self.config.REQUEST_TIMEOUT)
            await page.wait_for_timeout(page_config['wait_time'])
            
            title = await page.title()
            
            # æå–é¡µé¢å†…å®¹ - ç®€åŒ–ç‰ˆ
            content_data = await page.evaluate(f"""
                () => {{
                    const result = {{
                        text_content: '',
                        code_blocks: [],
                        metadata: {{}}
                    }};
                    
                    // æŸ¥æ‰¾ä¸»è¦å†…å®¹å®¹å™¨
                    const contentSelectors = {page_config['selectors']};
                    let contentContainer = document.body;
                    
                    for (const selector of contentSelectors) {{
                        const container = document.querySelector(selector);
                        if (container) {{
                            contentContainer = container;
                            break;
                        }}
                    }}
                    
                    // æå–æ–‡æœ¬å†…å®¹
                    const textElements = {self.config.TEXT_ELEMENTS};
                    const textParts = [];
                    
                    textElements.forEach(tagName => {{
                        const elements = contentContainer.querySelectorAll(tagName);
                        elements.forEach(el => {{
                            const text = el.textContent.trim();
                            if (text && text.length > {self.config.MIN_TEXT_LENGTH}) {{
                                textParts.push(text);
                            }}
                        }});
                    }});
                    
                    result.text_content = textParts.join('\\n\\n');
                    
                    // æå–ä»£ç å—
                    const codeSelectors = {page_config['code_selectors']};
                    let codeIndex = 0;
                    
                    codeSelectors.forEach(selector => {{
                        try {{
                            const codeElements = document.querySelectorAll(selector);
                            codeElements.forEach(codeEl => {{
                                const code = codeEl.textContent.trim();
                                if (code && code.length > {self.config.MIN_CODE_LENGTH}) {{
                                    let language = 'unknown';
                                    const classMatch = codeEl.className.match(/(?:language-|lang-)([\\w]+)/);
                                    if (classMatch) {{
                                        language = classMatch[1];
                                    }}
                                    
                                    result.code_blocks.push({{
                                        id: codeIndex++,
                                        language: language,
                                        code: code
                                    }});
                                }}
                            }});
                        }} catch(e) {{
                            console.log('Invalid selector:', selector);
                        }}
                    }});
                    
                    // å…ƒæ•°æ®
                    result.metadata['page_url'] = window.location.href;
                    result.metadata['page_title'] = document.title;
                    
                    return result;
                }}
            """)
            
            # æ£€æŸ¥å†…å®¹è´¨é‡
            total_content_length = len(content_data['text_content'])
            has_meaningful_content = (
                total_content_length >= self.config.FILTER_CONFIG['min_content_length'] or
                len(content_data['code_blocks']) > 0
            )
            
            if self.config.FILTER_CONFIG['skip_empty_content'] and not has_meaningful_content:
                content_details = f"æ–‡æœ¬: {total_content_length} å­—ç¬¦, ä»£ç : {len(content_data['code_blocks'])} ä¸ª"
                logger.debug(f"â© é¡µé¢å†…å®¹ä¸è¶³ï¼Œè·³è¿‡: {url} ({content_details})")
                return ContentExtractionResult.insufficient_content(url, content_details)
            
            # åˆ›å»ºPageContentå¯¹è±¡
            page_content = PageContent(
                url=url,
                title=title,
                text_content=content_data['text_content'],
                code_blocks=content_data['code_blocks'],
                metadata=content_data['metadata'],
                page_type=page_type
            )
            
            logger.info(f"âœ… æˆåŠŸæå–: {url} (ç±»å‹: {page_type}, å†…å®¹: {total_content_length} å­—ç¬¦)")
            return ContentExtractionResult.success_result(page_content)
            
        except Exception as e:
            logger.warning(f"âš ï¸ é¡µé¢è®¿é—®å¼‚å¸¸ {url}: {e}")
            return ContentExtractionResult.technical_error(str(e))
    
    async def crawl_with_retry(self, page: Page, url_info: Dict) -> Optional[PageContent]:
        """å¸¦é‡è¯•æœºåˆ¶çš„çˆ¬å–"""
        url = url_info['url']
        max_retries = self.config.MAX_RETRIES
        
        for attempt in range(max_retries + 1):
            try:
                result = await self.extract_content(page, url)
                
                if result.success:
                    return result.content
                elif not result.should_retry:
                    self.skipped_urls.append({
                        'url': url,
                        'reason': result.reason,
                        'skip_type': 'insufficient_content'
                    })
                    return None
                else:
                    if attempt < max_retries:
                        logger.warning(f"ğŸ”„ ç¬¬ {attempt + 1} æ¬¡æŠ€æœ¯é”™è¯¯ï¼Œå‡†å¤‡é‡è¯•: {url}")
                        await asyncio.sleep(self.config.RETRY_DELAY)
                    else:
                        self.failed_urls.append({
                            'url': url,
                            'error': result.reason,
                            'attempts': attempt + 1,
                            'failure_type': 'technical_error'
                        })
                        return None
                    
            except Exception as e:
                if attempt < max_retries:
                    logger.warning(f"ğŸ”„ ç¬¬ {attempt + 1} æ¬¡ä»£ç å¼‚å¸¸ï¼Œå‡†å¤‡é‡è¯• {url}: {e}")
                    await asyncio.sleep(self.config.RETRY_DELAY)
                else:
                    self.failed_urls.append({
                        'url': url,
                        'error': str(e),
                        'attempts': attempt + 1,
                        'failure_type': 'code_exception'
                    })
                    return None
        
        return None
    
    async def crawl_content(self, urls: List[Dict]) -> Dict[str, PageContent]:
        """æ‰¹é‡çˆ¬å–é¡µé¢å†…å®¹"""
        logger.info(f"å¼€å§‹çˆ¬å– {len(urls)} ä¸ªé¡µé¢çš„å†…å®¹")
        
        self.load_existing_content()
        
        # è¿‡æ»¤å·²çˆ¬å–çš„URL
        urls_to_crawl = [url_info for url_info in urls 
                        if not self.should_skip_url(url_info['url'])]
        already_crawled = len(urls) - len(urls_to_crawl)
        
        if already_crawled > 0:
            logger.info(f"ğŸ“‚ å‘ç° {already_crawled} ä¸ªå·²çˆ¬å–çš„é¡µé¢ï¼Œè·³è¿‡")
        
        if not urls_to_crawl:
            logger.info("âœ… æ‰€æœ‰é¡µé¢éƒ½å·²çˆ¬å–å®Œæˆ")
            return self.crawled_content
        
        logger.info(f"ğŸ“‹ éœ€è¦çˆ¬å– {len(urls_to_crawl)} ä¸ªæ–°é¡µé¢")
        
        async with async_playwright() as playwright:
            browser = await playwright.chromium.launch(
                headless=True,
                args=self.config.BROWSER_ARGS
            )
            
            try:
                semaphore = asyncio.Semaphore(self.config.MAX_CONCURRENT)
                
                async def crawl_single_page(url_info: Dict):
                    async with semaphore:
                        if self.should_skip_url(url_info['url']):
                            return
                        
                        page = await self.setup_page(browser)
                        try:
                            content = await self.crawl_with_retry(page, url_info)
                            if content:
                                self.crawled_content[url_info['url']] = content
                                await self.incremental_save()
                            
                            await asyncio.sleep(self.config.DELAY_SECONDS)
                            
                        finally:
                            await page.close()
                
                # å¹¶å‘çˆ¬å–
                tasks = [crawl_single_page(url_info) for url_info in urls_to_crawl]
                await asyncio.gather(*tasks, return_exceptions=True)
                
                # æœ€ç»ˆä¿å­˜
                await self.incremental_save(force=True)
                
                logger.info(f"çˆ¬å–å®Œæˆ: æˆåŠŸ {len(self.crawled_content)} ä¸ªï¼Œ"
                          f"å¤±è´¥ {len(self.failed_urls)} ä¸ªï¼Œè·³è¿‡ {len(self.skipped_urls)} ä¸ª")
                
                return self.crawled_content
                
            finally:
                await browser.close()
    
    async def incremental_save(self, force: bool = False):
        """å¢é‡ä¿å­˜"""
        if not self.enable_incremental_save:
            return
            
        async with self._save_lock:
            current_count = len(self.crawled_content)
            
            if not force and (current_count - self.last_save_count) < self.save_interval:
                return
            
            try:
                # ä¿å­˜åˆ° processed ç›®å½•
                output_path = self.config.PROCESSED_DATA_DIR / self.config.OUTPUT_FILE
                temp_path = output_path.with_suffix('.tmp')
                
                serializable_content = {}
                for url, content in self.crawled_content.items():
                    serializable_content[url] = asdict(content)
                
                with open(temp_path, 'w', encoding='utf-8') as f:
                    json.dump(serializable_content, f, ensure_ascii=False, indent=2)
                
                temp_path.replace(output_path)
                
                # ä¿å­˜å¤±è´¥å’Œè·³è¿‡è®°å½•
                if self.failed_urls:
                    failed_path = output_path.with_name(f"{output_path.stem}_failed.json")
                    with open(failed_path, 'w', encoding='utf-8') as f:
                        json.dump(self.failed_urls, f, ensure_ascii=False, indent=2)
                
                if self.skipped_urls:
                    skipped_path = output_path.with_name(f"{output_path.stem}_skipped.json")
                    with open(skipped_path, 'w', encoding='utf-8') as f:
                        json.dump(self.skipped_urls, f, ensure_ascii=False, indent=2)
                
                logger.info(f"ğŸ’¾ å¢é‡ä¿å­˜: {current_count} ä¸ªé¡µé¢å·²ä¿å­˜")
                self.last_save_count = current_count
                
            except Exception as e:
                logger.error(f"âŒ å¢é‡ä¿å­˜å¤±è´¥: {e}")
    
    def load_existing_content(self):
        """åŠ è½½å·²å­˜åœ¨çš„å†…å®¹"""
        output_path = self.config.PROCESSED_DATA_DIR / self.config.OUTPUT_FILE
        
        if not output_path.exists():
            logger.info("ğŸ“‚ æœªæ‰¾åˆ°ç°æœ‰å†…å®¹æ–‡ä»¶ï¼Œä»å¤´å¼€å§‹çˆ¬å–")
            return
        
        try:
            with open(output_path, 'r', encoding='utf-8') as f:
                existing_data = json.load(f)
            
            for url, data in existing_data.items():
                self.crawled_content[url] = PageContent(
                    url=data['url'],
                    title=data['title'],
                    text_content=data.get('text_content', ''),
                    code_blocks=data.get('code_blocks', []),
                    metadata=data.get('metadata', {}),
                    crawl_time=data.get('crawl_time', ''),
                    page_type=data.get('page_type', 'unknown'),
                    content_length=data.get('content_length', 0)
                )
            
            logger.info(f"ğŸ“‚ åŠ è½½ç°æœ‰å†…å®¹: {len(self.crawled_content)} ä¸ªé¡µé¢å·²å­˜åœ¨")
            self.last_save_count = len(self.crawled_content)
            
        except Exception as e:
            logger.error(f"âŒ åŠ è½½ç°æœ‰å†…å®¹å¤±è´¥: {e}")
    
    def should_skip_url(self, url: str) -> bool:
        """æ£€æŸ¥URLæ˜¯å¦åº”è¯¥è·³è¿‡"""
        if url in self.crawled_content:
            return True
        
        for skipped in self.skipped_urls:
            if skipped.get('url') == url:
                return True
        
        return False
    
    def save_content(self, filename: str = None):
        """ä¿å­˜çˆ¬å–çš„å†…å®¹"""
        if filename is None:
            filename = self.config.PROCESSED_DATA_DIR / self.config.OUTPUT_FILE
        
        try:
            serializable_content = {}
            for url, content in self.crawled_content.items():
                serializable_content[url] = asdict(content)
            
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(serializable_content, f, ensure_ascii=False, indent=2)
            
            logger.info(f"å†…å®¹å·²ä¿å­˜åˆ°: {filename}")
            
        except Exception as e:
            logger.error(f"ä¿å­˜å†…å®¹å¤±è´¥: {e}")
    
    def generate_statistics(self) -> Dict:
        """ç”Ÿæˆçˆ¬å–ç»Ÿè®¡ä¿¡æ¯"""
        total_pages = len(self.crawled_content)
        total_text_length = sum(len(content.text_content) for content in self.crawled_content.values())
        total_code_blocks = sum(len(content.code_blocks) for content in self.crawled_content.values())
        
        return {
            'total_pages': total_pages,
            'total_text_length': total_text_length,
            'total_code_blocks': total_code_blocks,
            'failed_pages': len(self.failed_urls),
            'skipped_pages': len(self.skipped_urls),
            'crawl_time': datetime.now().isoformat()
        } 