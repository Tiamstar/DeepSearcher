#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
åä¸ºæ–‡æ¡£DeepSearcheré€‚é…å™¨ - é‡æ„ç‰ˆ
å°†åä¸ºæ–‡æ¡£çˆ¬è™«ç»“æœé€‚é…åˆ°DeepSearcheræ¡†æ¶
"""

import json
import os
import hashlib
import logging
from typing import List, Dict, Any, Optional
from dataclasses import dataclass, field
from pathlib import Path

from langchain_core.documents import Document
from deepsearcher.configuration import vector_db, embedding_model
from deepsearcher.loader.splitter import Chunk, split_docs_to_chunks

from .config import RAGConfig

logger = logging.getLogger(__name__)

@dataclass
class HuaweiDocument:
    """åä¸ºæ–‡æ¡£æ•°æ®ç»“æ„"""
    url: str
    title: str
    content: str
    content_type: str  # 'text', 'code'
    language: str = "unknown"
    metadata: Dict[str, Any] = field(default_factory=dict)
    
    def __post_init__(self):
        if self.metadata is None:
            self.metadata = {}


class HuaweiDeepSearcherAdapter:
    """åä¸ºæ–‡æ¡£DeepSearcheré€‚é…å™¨ - é‡æ„ç‰ˆ"""
    
    def __init__(self, 
                 content_file: str = None,
                 collection_name: str = None,
                 chunk_size: int = None,
                 chunk_overlap: int = None,
                 content_type: str = "auto"):  # æ–°å¢ï¼šå†…å®¹ç±»å‹é€‰æ‹©
        """
        åˆå§‹åŒ–åä¸ºæ–‡æ¡£é€‚é…å™¨
        
        Args:
            content_file: æŒ‡å®šçš„å†…å®¹æ–‡ä»¶è·¯å¾„
            collection_name: é›†åˆåç§°
            chunk_size: åˆ†å—å¤§å°
            chunk_overlap: åˆ†å—é‡å å¤§å°
            content_type: å†…å®¹ç±»å‹é€‰æ‹© ("auto", "expanded", "basic", "all")
                - "auto": è‡ªåŠ¨é€‰æ‹©æœ€æ–°çš„æ‰©å±•å†…å®¹æ–‡ä»¶
                - "expanded": ä¼˜å…ˆé€‰æ‹©æ‰©å±•å†…å®¹æ–‡ä»¶
                - "basic": é€‰æ‹©åŸºç¡€å†…å®¹æ–‡ä»¶ (huawei_docs_content.json)
                - "all": åˆå¹¶æ‰€æœ‰å†…å®¹æ–‡ä»¶
        """
        
        # ä½¿ç”¨é…ç½®é»˜è®¤å€¼
        self.content_type = content_type
        self.content_file = Path(content_file) if content_file else self._find_content_file(content_type)
        self.collection_name = (collection_name or RAGConfig.DEFAULT_COLLECTION_NAME).replace(" ", "_").replace("-", "_")
        self.chunk_size = chunk_size or RAGConfig.DEFAULT_CHUNK_SIZE
        self.chunk_overlap = chunk_overlap or RAGConfig.DEFAULT_CHUNK_OVERLAP
        
        # å»¶è¿Ÿåˆå§‹åŒ–å…¨å±€ç»„ä»¶
        self._vector_db = None
        self._embedding_model = None
        
        logger.info(f"âœ… åä¸ºæ–‡æ¡£é€‚é…å™¨åˆå§‹åŒ–å®Œæˆ")
        logger.info(f"   é›†åˆåç§°: {self.collection_name}")
        logger.info(f"   å†…å®¹ç±»å‹: {self.content_type}")
        logger.info(f"   å†…å®¹æ–‡ä»¶: {self.content_file}")
    
    @property
    def vector_db(self):
        """å»¶è¿Ÿè·å–å‘é‡æ•°æ®åº“"""
        if self._vector_db is None:
            from deepsearcher.configuration import vector_db
            if vector_db is None:
                raise RuntimeError("å‘é‡æ•°æ®åº“æœªåˆå§‹åŒ–ï¼Œè¯·å…ˆè°ƒç”¨ init_config()")
            self._vector_db = vector_db
            logger.info(f"   å‘é‡æ•°æ®åº“: {type(self._vector_db).__name__}")
        return self._vector_db
    
    @property
    def embedding_model(self):
        """å»¶è¿Ÿè·å–åµŒå…¥æ¨¡å‹"""
        if self._embedding_model is None:
            from deepsearcher.configuration import embedding_model
            if embedding_model is None:
                raise RuntimeError("åµŒå…¥æ¨¡å‹æœªåˆå§‹åŒ–ï¼Œè¯·å…ˆè°ƒç”¨ init_config()")
            self._embedding_model = embedding_model
            logger.info(f"   åµŒå…¥æ¨¡å‹: {type(self._embedding_model).__name__}")
        return self._embedding_model
    
    def _find_content_file(self, content_type: str = "auto") -> Path:
        """æ ¹æ®å†…å®¹ç±»å‹æŸ¥æ‰¾åä¸ºæ–‡æ¡£å†…å®¹æ–‡ä»¶"""
        data_dir = Path("data/processed")
        
        if content_type == "basic":
            # é€‰æ‹©åŸºç¡€å†…å®¹æ–‡ä»¶
            basic_file = data_dir / "huawei_docs_content.json"
            if basic_file.exists():
                logger.info(f"ğŸ” é€‰æ‹©åŸºç¡€å†…å®¹æ–‡ä»¶: {basic_file}")
                return basic_file
            else:
                logger.warning(f"âš ï¸ åŸºç¡€å†…å®¹æ–‡ä»¶ä¸å­˜åœ¨: {basic_file}")
        
        elif content_type == "expanded":
            # ä¼˜å…ˆé€‰æ‹©æ‰©å±•å†…å®¹æ–‡ä»¶
            expanded_files = [f for f in data_dir.glob("huawei_docs_expanded_content_*.json") 
                             if not f.name.endswith('_stats.json') and not f.name.endswith('_failed.json')]
            if expanded_files:
                latest_file = max(expanded_files, key=lambda x: x.stat().st_mtime)
                logger.info(f"ğŸ” é€‰æ‹©æ‰©å±•å†…å®¹æ–‡ä»¶: {latest_file}")
                return latest_file
            else:
                logger.warning(f"âš ï¸ æœªæ‰¾åˆ°æ‰©å±•å†…å®¹æ–‡ä»¶")
        
        elif content_type == "all":
            # è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬è¿”å›ä¸€ä¸ªæ ‡è®°ï¼Œåç»­åœ¨load_huawei_contentä¸­å¤„ç†
            logger.info(f"ğŸ” å°†åˆå¹¶æ‰€æœ‰å¯ç”¨çš„å†…å®¹æ–‡ä»¶")
            return data_dir / "ALL_CONTENT_FILES"
        
        # content_type == "auto" æˆ–å…¶ä»–æƒ…å†µçš„é»˜è®¤é€»è¾‘
        # æŸ¥æ‰¾æ‰©å±•å†…å®¹æ–‡ä»¶ï¼ˆæ’é™¤ç»Ÿè®¡æ–‡ä»¶å’Œå¤±è´¥æ–‡ä»¶ï¼‰
        expanded_files = [f for f in data_dir.glob("huawei_docs_expanded_content_*.json") 
                         if not f.name.endswith('_stats.json') and not f.name.endswith('_failed.json')]
        if expanded_files:
            latest_file = max(expanded_files, key=lambda x: x.stat().st_mtime)
            logger.info(f"ğŸ” è‡ªåŠ¨é€‰æ‹©æœ€æ–°æ‰©å±•å†…å®¹æ–‡ä»¶: {latest_file}")
            return latest_file
        
        # æŸ¥æ‰¾æ™®é€šå†…å®¹æ–‡ä»¶ï¼ˆæ’é™¤ç»Ÿè®¡æ–‡ä»¶å’Œå¤±è´¥æ–‡ä»¶ï¼‰
        content_files = [f for f in data_dir.glob("huawei_docs_content*.json") 
                        if not f.name.endswith('_stats.json') and not f.name.endswith('_failed.json')]
        if content_files:
            latest_file = max(content_files, key=lambda x: x.stat().st_mtime)
            logger.info(f"ğŸ” æ‰¾åˆ°åŸºç¡€å†…å®¹æ–‡ä»¶: {latest_file}")
            return latest_file
        
        # é»˜è®¤è·¯å¾„
        default_path = data_dir / "huawei_docs_content.json"
        logger.warning(f"âš ï¸ æœªæ‰¾åˆ°åä¸ºæ–‡æ¡£å†…å®¹æ–‡ä»¶ï¼Œä½¿ç”¨é»˜è®¤è·¯å¾„: {default_path}")
        return default_path
    
    def list_available_content_files(self) -> Dict[str, Any]:
        """åˆ—å‡ºæ‰€æœ‰å¯ç”¨çš„å†…å®¹æ–‡ä»¶"""
        data_dir = Path("data/processed")
        
        files_info = {
            'expanded_files': [],
            'basic_files': [],
            'other_files': []
        }
        
        # æŸ¥æ‰¾æ‰©å±•å†…å®¹æ–‡ä»¶
        expanded_files = [f for f in data_dir.glob("huawei_docs_expanded_content_*.json") 
                         if not f.name.endswith('_stats.json') and not f.name.endswith('_failed.json')]
        for file in expanded_files:
            files_info['expanded_files'].append({
                'name': file.name,
                'path': str(file),
                'size_mb': round(file.stat().st_size / 1024 / 1024, 2),
                'modified': file.stat().st_mtime
            })
        
        # æŸ¥æ‰¾åŸºç¡€å†…å®¹æ–‡ä»¶
        basic_files = [f for f in data_dir.glob("huawei_docs_content*.json") 
                      if not f.name.startswith('huawei_docs_expanded_content') 
                      and not f.name.endswith('_stats.json') 
                      and not f.name.endswith('_failed.json')]
        for file in basic_files:
            files_info['basic_files'].append({
                'name': file.name,
                'path': str(file),
                'size_mb': round(file.stat().st_size / 1024 / 1024, 2),
                'modified': file.stat().st_mtime
            })
        
        # æŸ¥æ‰¾å…¶ä»–ç›¸å…³æ–‡ä»¶
        other_files = [f for f in data_dir.glob("huawei_docs_*.json") 
                      if not any(f.name.startswith(prefix) for prefix in 
                               ['huawei_docs_content', 'huawei_docs_expanded_content'])
                      and not f.name.endswith('_stats.json') 
                      and not f.name.endswith('_failed.json')]
        for file in other_files:
            files_info['other_files'].append({
                'name': file.name,
                'path': str(file),
                'size_mb': round(file.stat().st_size / 1024 / 1024, 2),
                'modified': file.stat().st_mtime
            })
        
        return files_info
    
    def load_huawei_content(self) -> Dict[str, Any]:
        """åŠ è½½åä¸ºæ–‡æ¡£å†…å®¹ - æ”¯æŒå¤šæ–‡ä»¶åˆå¹¶ï¼Œå¢å¼ºç¼–ç å¤„ç†"""
        try:
            content_path = Path(self.content_file)
            
            # ç‰¹æ®Šå¤„ç†ï¼šåˆå¹¶æ‰€æœ‰å†…å®¹æ–‡ä»¶
            if content_path.name == "ALL_CONTENT_FILES":
                return self._load_merged_content()
            
            # å¸¸è§„å•æ–‡ä»¶åŠ è½½
            if not content_path.exists():
                raise FileNotFoundError(f"åä¸ºæ–‡æ¡£å†…å®¹æ–‡ä»¶ä¸å­˜åœ¨: {content_path}")
            
            # å¢å¼ºçš„æ–‡ä»¶è¯»å–ï¼Œæ”¯æŒå¤šç§ç¼–ç 
            content = None
            encodings = ['utf-8', 'utf-8-sig', 'gbk', 'gb2312', 'latin1']
            
            for encoding in encodings:
                try:
                    with open(content_path, 'r', encoding=encoding) as f:
                        content = json.load(f)
                    logger.info(f"âœ… ä½¿ç”¨ {encoding} ç¼–ç æˆåŠŸè¯»å–æ–‡ä»¶")
                    break
                except UnicodeDecodeError:
                    continue
                except Exception as e:
                    if encoding == encodings[-1]:  # æœ€åä¸€ä¸ªç¼–ç ä¹Ÿå¤±è´¥
                        raise e
                    continue
            
            if content is None:
                raise Exception("æ— æ³•ä½¿ç”¨ä»»ä½•æ”¯æŒçš„ç¼–ç è¯»å–æ–‡ä»¶")
            
            logger.info(f"ğŸ“š æˆåŠŸåŠ è½½ {len(content)} ä¸ªé¡µé¢çš„åä¸ºæ–‡æ¡£å†…å®¹")
            logger.info(f"   æ–‡ä»¶: {content_path.name}")
            logger.info(f"   å¤§å°: {content_path.stat().st_size / 1024 / 1024:.2f} MB")
            
            return content
            
        except Exception as e:
            logger.error(f"âŒ åŠ è½½åä¸ºæ–‡æ¡£å†…å®¹å¤±è´¥: {e}")
            return {}
    
    def _load_merged_content(self) -> Dict[str, Any]:
        """åˆå¹¶åŠ è½½æ‰€æœ‰å¯ç”¨çš„å†…å®¹æ–‡ä»¶ï¼Œå¢å¼ºç¼–ç å¤„ç†"""
        logger.info("ğŸ”„ å¼€å§‹åˆå¹¶åŠ è½½æ‰€æœ‰å†…å®¹æ–‡ä»¶...")
        
        data_dir = Path("data/processed")
        merged_content = {}
        loaded_files = []
        
        def safe_load_json(file_path: Path) -> Dict[str, Any]:
            """å®‰å…¨åŠ è½½JSONæ–‡ä»¶ï¼Œæ”¯æŒå¤šç§ç¼–ç """
            encodings = ['utf-8', 'utf-8-sig', 'gbk', 'gb2312', 'latin1']
            
            for encoding in encodings:
                try:
                    with open(file_path, 'r', encoding=encoding) as f:
                        return json.load(f)
                except UnicodeDecodeError:
                    continue
                except Exception as e:
                    if encoding == encodings[-1]:
                        raise e
                    continue
            
            raise Exception(f"æ— æ³•ä½¿ç”¨ä»»ä½•æ”¯æŒçš„ç¼–ç è¯»å–æ–‡ä»¶: {file_path}")
        
        # 1. åŠ è½½åŸºç¡€å†…å®¹æ–‡ä»¶
        basic_file = data_dir / "huawei_docs_content.json"
        if basic_file.exists():
            try:
                basic_content = safe_load_json(basic_file)
                merged_content.update(basic_content)
                loaded_files.append(f"åŸºç¡€å†…å®¹: {basic_file.name} ({len(basic_content)} é¡µ)")
                logger.info(f"   âœ… åŠ è½½åŸºç¡€å†…å®¹: {len(basic_content)} é¡µ")
            except Exception as e:
                logger.warning(f"   âš ï¸ åŠ è½½åŸºç¡€å†…å®¹å¤±è´¥: {e}")
        
        # 2. åŠ è½½æ‰©å±•å†…å®¹æ–‡ä»¶ï¼ˆåªåŠ è½½ä¸é‡å¤çš„å†…å®¹ï¼‰
        expanded_files = [f for f in data_dir.glob("huawei_docs_expanded_content_*.json") 
                         if not f.name.endswith('_stats.json') and not f.name.endswith('_failed.json')]
        
        for expanded_file in expanded_files:
            try:
                expanded_content = safe_load_json(expanded_file)
                
                # åªæ·»åŠ æ–°çš„URLï¼Œé¿å…é‡å¤
                new_urls = 0
                for url, content in expanded_content.items():
                    if url not in merged_content:
                        merged_content[url] = content
                        new_urls += 1
                
                loaded_files.append(f"æ‰©å±•å†…å®¹: {expanded_file.name} (+{new_urls} æ–°é¡µ)")
                logger.info(f"   âœ… åŠ è½½æ‰©å±•å†…å®¹: +{new_urls} æ–°é¡µé¢")
                
            except Exception as e:
                logger.warning(f"   âš ï¸ åŠ è½½æ‰©å±•å†…å®¹å¤±è´¥ {expanded_file.name}: {e}")
        
        logger.info(f"ğŸ“Š åˆå¹¶å®Œæˆ:")
        logger.info(f"   æ€»é¡µé¢æ•°: {len(merged_content)}")
        logger.info(f"   åŠ è½½çš„æ–‡ä»¶:")
        for file_info in loaded_files:
            logger.info(f"     - {file_info}")
        
        return merged_content
    
    def create_document_id(self, url: str, chunk_index: int, content_type: str) -> str:
        """åˆ›å»ºæ–‡æ¡£ID"""
        content_hash = hashlib.md5(f"{url}_{chunk_index}_{content_type}".encode()).hexdigest()[:8]
        return f"huawei_{content_type}_{content_hash}_{chunk_index}"
    
    def split_text_smartly(self, text: str, chunk_size: int, overlap: int) -> List[str]:
        """æ™ºèƒ½æ–‡æœ¬åˆ†å— - ä¼˜åŒ–çš„ä¸­æ–‡åˆ†å‰²"""
        if len(text) <= chunk_size:
            return [text]
        
        chunks = []
        start = 0
        
        while start < len(text):
            end = start + chunk_size
            
            if end >= len(text):
                chunks.append(text[start:])
                break
            
            # å¯»æ‰¾å¥å­ç»“å°¾
            sentence_endings = ['ã€‚', 'ï¼', 'ï¼Ÿ', '\n\n', 'ï¼›', '. ', '! ', '? ', '\n\n']
            best_split = end
            
            for i in range(end - overlap, end):
                if i > start:
                    for ending in sentence_endings:
                        if text[i:i+len(ending)] == ending:
                            best_split = i + len(ending)
                            break
                    if best_split != end:
                        break
            
            chunks.append(text[start:best_split])
            start = best_split - overlap if best_split > start + overlap else best_split
        
        return chunks
    
    def process_page_content(self, url: str, page_data: Dict[str, Any]) -> List[HuaweiDocument]:
        """å¤„ç†å•ä¸ªé¡µé¢å†…å®¹ï¼Œç”Ÿæˆå¤šä¸ªæ–‡æ¡£å¯¹è±¡ï¼Œå¢å¼ºç¼–ç å¤„ç†"""
        documents = []
        title = self._safe_decode_text(page_data.get('title', 'Unknown'))
        page_type = page_data.get('page_type', 'unknown')
        
        # 1. å¤„ç†ä¸»è¦æ–‡æœ¬å†…å®¹
        text_content = self._safe_decode_text(page_data.get('text_content', '')).strip()
        if text_content and len(text_content) > 50:
            if len(text_content) > self.chunk_size:
                text_chunks = self.split_text_smartly(text_content, self.chunk_size, self.chunk_overlap)
                for i, chunk in enumerate(text_chunks):
                    documents.append(HuaweiDocument(
                        url=url,
                        title=f"{title} (ç¬¬{i+1}éƒ¨åˆ†)",
                        content=chunk,
                        content_type='text',
                        metadata={
                            'chunk_index': i,
                            'total_chunks': len(text_chunks),
                            'page_type': page_type,
                            'original_title': title,
                            'original_url': url
                        }
                    ))
            else:
                documents.append(HuaweiDocument(
                    url=url,
                    title=title,
                    content=text_content,
                    content_type='text',
                    metadata={
                        'page_type': page_type,
                        'original_title': title,
                        'original_url': url
                    }
                ))
        
        # 2. å¤„ç†ä»£ç å—
        code_blocks = page_data.get('code_blocks', [])
        for i, code_block in enumerate(code_blocks):
            code_content = self._safe_decode_text(code_block.get('code', '')).strip()
            if not code_content or len(code_content) < 20:
                continue
                
            language = code_block.get('language', 'unknown')
            
            # æ„å»ºä»£ç å†…å®¹
            enhanced_content = f"ä»£ç è¯­è¨€: {language}\nä»£ç å†…å®¹:\n{code_content}"
            
            documents.append(HuaweiDocument(
                url=url,
                title=f"{title} - ä»£ç ç¤ºä¾‹ {i+1}",
                content=enhanced_content,
                content_type='code',
                language=language,
                metadata={
                    'code_index': i,
                    'code_language': language,
                    'raw_code': code_content,
                    'page_type': page_type,
                    'original_title': title,
                    'original_url': url
                }
            ))
        
        return documents
    
    def convert_to_langchain_documents(self, huawei_docs: List[HuaweiDocument]) -> List[Document]:
        """å°†HuaweiDocumentè½¬æ¢ä¸ºLangChain Documentæ ¼å¼"""
        langchain_docs = []
        
        for doc in huawei_docs:
            metadata = {
                **doc.metadata,
                'content_type': doc.content_type,
                'language': doc.language,
                'document_id': self.create_document_id(doc.url, 
                                                     doc.metadata.get('chunk_index', 0), 
                                                     doc.content_type),
                'title': doc.title,
                'url': doc.url,
                'reference': f"{doc.title} ({doc.url})"
            }
            
            langchain_doc = Document(
                page_content=doc.content,
                metadata=metadata
            )
            
            langchain_docs.append(langchain_doc)
        
        return langchain_docs
    
    def load_huawei_documents(self) -> List[HuaweiDocument]:
        """åŠ è½½åä¸ºæ–‡æ¡£å†…å®¹å¹¶è½¬æ¢ä¸ºHuaweiDocumentå¯¹è±¡åˆ—è¡¨"""
        try:
            # 1. åŠ è½½åŸå§‹å†…å®¹
            raw_content = self.load_huawei_content()
            if not raw_content:
                logger.error("âŒ æ²¡æœ‰åŠ è½½åˆ°ä»»ä½•å†…å®¹")
                return []
            
            logger.info(f"ğŸ“š å¼€å§‹å¤„ç† {len(raw_content)} ä¸ªé¡µé¢")
            
            # 2. å¤„ç†æ¯ä¸ªé¡µé¢ï¼Œè½¬æ¢ä¸ºHuaweiDocumentå¯¹è±¡
            all_documents = []
            processed_count = 0
            
            for url, page_data in raw_content.items():
                try:
                    # å¤„ç†å•ä¸ªé¡µé¢å†…å®¹
                    page_documents = self.process_page_content(url, page_data)
                    all_documents.extend(page_documents)
                    processed_count += 1
                    
                    # å®šæœŸæ˜¾ç¤ºè¿›åº¦
                    if processed_count % 100 == 0:
                        logger.info(f"   ğŸ“„ å·²å¤„ç† {processed_count}/{len(raw_content)} ä¸ªé¡µé¢")
                        
                except Exception as e:
                    logger.warning(f"âš ï¸ å¤„ç†é¡µé¢å¤±è´¥ {url}: {e}")
                    continue
            
            logger.info(f"âœ… æ–‡æ¡£å¤„ç†å®Œæˆ:")
            logger.info(f"   ğŸ“„ å¤„ç†é¡µé¢æ•°: {processed_count}")
            logger.info(f"   ğŸ“ ç”Ÿæˆæ–‡æ¡£æ•°: {len(all_documents)}")
            
            # 3. ç»Ÿè®¡ä¿¡æ¯
            text_docs = sum(1 for doc in all_documents if doc.content_type == 'text')
            code_docs = sum(1 for doc in all_documents if doc.content_type == 'code')
            
            logger.info(f"   ğŸ“ƒ æ–‡æœ¬æ–‡æ¡£: {text_docs}")
            logger.info(f"   ğŸ’» ä»£ç æ–‡æ¡£: {code_docs}")
            
            return all_documents
            
        except Exception as e:
            logger.error(f"âŒ åŠ è½½åä¸ºæ–‡æ¡£å¤±è´¥: {e}")
            import traceback
            logger.error(f"è¯¦ç»†é”™è¯¯ä¿¡æ¯: {traceback.format_exc()}")
            return []
    
    def load_to_vector_database(self, 
                               force_new_collection: bool = False,
                               incremental_update: bool = True,  # æ–°å¢ï¼šå¢é‡æ›´æ–°å‚æ•°
                               batch_size: int = 64) -> bool:
        """åŠ è½½åä¸ºæ–‡æ¡£åˆ°å‘é‡æ•°æ®åº“ - ä½¿ç”¨ç»Ÿä¸€çš„åŠ è½½å‡½æ•°"""
        try:
            logger.info("ğŸ’¾ å¼€å§‹åŠ è½½å†…å®¹åˆ°å‘é‡æ•°æ®åº“...")
            
            # 1. åŠ è½½æ–‡æ¡£å¹¶è½¬æ¢ä¸ºLangChainæ ¼å¼
            huawei_docs = self.load_huawei_documents()
            if not huawei_docs:
                logger.error("âŒ æ²¡æœ‰æ‰¾åˆ°å¯åŠ è½½çš„æ–‡æ¡£")
                return False
            
            logger.info(f"ğŸ“š åŠ è½½äº† {len(huawei_docs)} ä¸ªæ–‡æ¡£")
            
            # 2. è½¬æ¢ä¸ºLangChainæ–‡æ¡£æ ¼å¼
            langchain_docs = self.convert_to_langchain_documents(huawei_docs)
            
            # 3. ä¿å­˜ä¸ºä¸´æ—¶æ–‡ä»¶ä»¥ä¾›offline_loadingä½¿ç”¨
            import tempfile
            import json
            from pathlib import Path
            
            # åˆ›å»ºä¸´æ—¶æ–‡ä»¶æ¥å­˜å‚¨æ–‡æ¡£
            temp_dir = Path(tempfile.mkdtemp())
            temp_file = temp_dir / "temp_huawei_docs.json"
            
            # è½¬æ¢ä¸ºå¯åºåˆ—åŒ–æ ¼å¼
            serializable_docs = []
            for doc in langchain_docs:
                serializable_docs.append({
                    'page_content': doc.page_content,
                    'metadata': doc.metadata
                })
            
            # ä¿å­˜åˆ°ä¸´æ—¶æ–‡ä»¶
            with open(temp_file, 'w', encoding='utf-8') as f:
                json.dump(serializable_docs, f, ensure_ascii=False, indent=2)
            
            logger.info(f"ğŸ“„ åˆ›å»ºä¸´æ—¶æ–‡ä»¶: {temp_file}")
            
            try:
                # 4. ä½¿ç”¨ç»Ÿä¸€çš„offline_loadingå‡½æ•°
                from deepsearcher.offline_loading import load_from_local_files
                
                # åˆ›å»ºè‡ªå®šä¹‰æ–‡ä»¶åŠ è½½å™¨ï¼Œä»JSONæ–‡ä»¶åŠ è½½æˆ‘ä»¬çš„æ–‡æ¡£
                from deepsearcher.configuration import file_loader
                
                # ä¸´æ—¶æ›¿æ¢file_loaderçš„load_fileæ–¹æ³•
                original_load_file = file_loader.load_file
                
                def custom_load_file(file_path: str):
                    """è‡ªå®šä¹‰æ–‡ä»¶åŠ è½½å™¨ï¼Œä»JSONåŠ è½½è½¬æ¢åçš„æ–‡æ¡£"""
                    if str(file_path) == str(temp_file):
                        from langchain_core.documents import Document
                        
                        with open(file_path, 'r', encoding='utf-8') as f:
                            data = json.load(f)
                        
                        documents = []
                        for item in data:
                            doc = Document(
                                page_content=item['page_content'],
                                metadata=item['metadata']
                            )
                            documents.append(doc)
                        
                        return documents
                    else:
                        return original_load_file(file_path)
                
                # ä¸´æ—¶æ›¿æ¢åŠ è½½å™¨
                file_loader.load_file = custom_load_file
                
                # è°ƒç”¨ç»Ÿä¸€çš„åŠ è½½å‡½æ•°
                load_from_local_files(
                    paths_or_directory=str(temp_file),
                    collection_name=self.collection_name,
                    collection_description=f"åä¸ºæ–‡æ¡£é›†åˆ - {self.content_type}",
                    force_new_collection=force_new_collection,
                    incremental_update=incremental_update,
                    chunk_size=self.chunk_size,
                    chunk_overlap=self.chunk_overlap,
                    batch_size=batch_size
                )
                
                # æ¢å¤åŸå§‹åŠ è½½å™¨
                file_loader.load_file = original_load_file
                
                logger.info("ğŸ‰ å‘é‡æ•°æ®åº“åŠ è½½å®Œæˆ!")
                return True
                
            finally:
                # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
                try:
                    if temp_file.exists():
                        temp_file.unlink()
                    if temp_dir.exists():
                        temp_dir.rmdir()
                    logger.info("ğŸ§¹ æ¸…ç†ä¸´æ—¶æ–‡ä»¶å®Œæˆ")
                except Exception as e:
                    logger.warning(f"âš ï¸ æ¸…ç†ä¸´æ—¶æ–‡ä»¶å¤±è´¥: {e}")
            
        except Exception as e:
            logger.error(f"âŒ åŠ è½½åˆ°å‘é‡æ•°æ®åº“å¤±è´¥: {e}")
            import traceback
            logger.error(f"è¯¦ç»†é”™è¯¯ä¿¡æ¯: {traceback.format_exc()}")
            return False
    
    def search_huawei_docs(self, 
                          query: str, 
                          top_k: int = 5,
                          content_type: str = None) -> List[Dict]:
        """æœç´¢åä¸ºæ–‡æ¡£ï¼Œå¢å¼ºç¼–ç å¤„ç†"""
        try:
            logger.info(f"ğŸ” æœç´¢åä¸ºæ–‡æ¡£: {query}")
            
            # ç”ŸæˆæŸ¥è¯¢å‘é‡
            query_vector = self.embedding_model.embed_query(query)
            
            # ä½¿ç”¨å‘é‡æ•°æ®åº“çš„æ­£ç¡®æœç´¢æ–¹æ³•
            results = self.vector_db.search_data(
                collection=self.collection_name,
                vector=query_vector,
                top_k=top_k * 2,  # å¤šè·å–ä¸€äº›ç»“æœç”¨äºè¿‡æ»¤
                query_text=query  # ä¼ é€’åŸå§‹æŸ¥è¯¢æ–‡æœ¬ï¼Œæ”¯æŒæ··åˆæœç´¢
            )
            
            if not results:
                logger.info("ğŸ“­ æ²¡æœ‰æ‰¾åˆ°ç›¸å…³ç»“æœ")
                return []
            
            # æ ¼å¼åŒ–ç»“æœ
            formatted_results = []
            for result in results:
                # è¿‡æ»¤å†…å®¹ç±»å‹
                if content_type and result.metadata.get('content_type') != content_type:
                    continue
                
                # å¢å¼ºçš„æ–‡æœ¬å†…å®¹å¤„ç†
                text_content = self._safe_decode_text(result.text)
                
                # å®‰å…¨è·å–å…ƒæ•°æ®
                metadata = getattr(result, 'metadata', {}) or {}
                title = self._safe_decode_text(metadata.get('title', 'Unknown'))
                url = metadata.get('url', '')
                
                formatted_result = {
                    'title': title,
                    'content': text_content,
                    'url': url,
                    'content_type': metadata.get('content_type', 'unknown'),
                    'language': metadata.get('language', 'unknown'),
                    'score': getattr(result, 'score', 0.0),
                    'metadata': metadata
                }
                formatted_results.append(formatted_result)
            
            # æŒ‰ç›¸å…³æ€§æ’åºå¹¶é™åˆ¶æ•°é‡
            formatted_results.sort(key=lambda x: x['score'])
            # final_results = formatted_results[0]
            # prompt=f"""
            # è¯·æ£€æŸ¥ä»¥ä¸‹å†…å®¹çš„æ­£ç¡®æ€§ï¼Œç”Ÿæˆä¸€ä¸ªç®€æ´çš„æ€»ç»“ï¼Œå­—æ•°ä¸è¶…è¿‡100å­—ã€‚
            # å†…å®¹:
            # {final_results}
            # æ€»ç»“:
            # """
            # è°ƒç”¨deepseek api
            # response = deepseek_api.chat.completions.create(
            #     model="deepseek-chat",
            #     messages=[{"role": "user", "content": prompt}]
            # )
            # è¿”å›æ€»ç»“
            # æ€»ç»“+æœ€åˆè¾“å…¥é€åˆ°deepseek api
            return formatted_results[:top_k]
            
        except Exception as e:
            logger.error(f"æœç´¢åä¸ºæ–‡æ¡£å¤±è´¥: {e}")
            import traceback
            logger.error(f"è¯¦ç»†é”™è¯¯ä¿¡æ¯: {traceback.format_exc()}")
            return []
    
    def _safe_decode_text(self, text_content: Any) -> str:
        """å®‰å…¨è§£ç æ–‡æœ¬å†…å®¹ï¼Œå¤„ç†å„ç§ç¼–ç é—®é¢˜"""
        if text_content is None:
            return ""
        
        # å¦‚æœå·²ç»æ˜¯å­—ç¬¦ä¸²ï¼Œç›´æ¥è¿”å›
        if isinstance(text_content, str):
            return text_content
        
        # å¦‚æœæ˜¯å­—èŠ‚ç±»å‹ï¼Œå°è¯•è§£ç 
        if isinstance(text_content, bytes):
            encodings = ['utf-8', 'utf-8-sig', 'gbk', 'gb2312', 'latin1']
            
            for encoding in encodings:
                try:
                    return text_content.decode(encoding)
                except UnicodeDecodeError:
                    continue
                except Exception:
                    continue
            
            # å¦‚æœæ‰€æœ‰ç¼–ç éƒ½å¤±è´¥ï¼Œä½¿ç”¨errors='ignore'
            try:
                return text_content.decode('utf-8', errors='ignore')
            except Exception:
                return str(text_content)
        
        # å…¶ä»–ç±»å‹è½¬æ¢ä¸ºå­—ç¬¦ä¸²
        try:
            return str(text_content)
        except Exception:
            return ""
    
    def get_collection_info(self) -> Dict[str, Any]:
        """è·å–é›†åˆä¿¡æ¯ - ä¿®å¤ç‰ˆï¼Œæ­£ç¡®å¤„ç†Milvusçš„ç»Ÿè®¡ä¿¡æ¯"""
        try:
            # å¯¹äºMilvusï¼Œä½¿ç”¨æ­£ç¡®çš„APIè°ƒç”¨
            vector_db_type = type(self.vector_db).__name__
            logger.debug(f"å‘é‡æ•°æ®åº“ç±»å‹: {vector_db_type}")
            
            if "Milvus" in vector_db_type:
                try:
                    # ä½¿ç”¨MilvusClientçš„has_collectionæ–¹æ³•
                    has_collection = self.vector_db.client.has_collection(self.collection_name)
                    if not has_collection:
                        return {'exists': False, 'name': self.collection_name}
                    
                    # å°è¯•ç›´æ¥ä½¿ç”¨pymilvusè·å–å®ä½“æ•°é‡
                    try:
                        from pymilvus import Collection
                        collection = Collection(self.collection_name)
                        collection.load()  # ç¡®ä¿é›†åˆå·²åŠ è½½
                        
                        # è·å–å®ä½“æ•°é‡
                        num_entities = collection.num_entities
                        logger.debug(f"Milvusé›†åˆ {self.collection_name} å®ä½“æ•°é‡: {num_entities}")
                        
                        return {
                            'name': self.collection_name,
                            'count': num_entities,
                            'exists': True,
                            'type': 'Milvus'
                        }
                        
                    except Exception as milvus_error:
                        logger.warning(f"ç›´æ¥æŸ¥è¯¢Milvuså®ä½“æ•°é‡å¤±è´¥: {milvus_error}")
                        
                        # å›é€€æ–¹æ³•ï¼šå°è¯•æœç´¢æ¥åˆ¤æ–­æ˜¯å¦æœ‰æ•°æ®
                        try:
                            # åˆ›å»ºä¸€ä¸ªæµ‹è¯•å‘é‡
                            sample_vector = [0.0] * 768  # å‡è®¾768ç»´ï¼Œåé¢ä¼šç”¨å®é™…ç»´åº¦
                            
                            # å°è¯•è·å–å®é™…çš„åµŒå…¥ç»´åº¦
                            try:
                                test_embedding = self.embedding_model.embed_query("test")
                                sample_vector = [0.0] * len(test_embedding)
                            except Exception:
                                # å¦‚æœæ— æ³•è·å–å®é™…ç»´åº¦ï¼Œä½¿ç”¨å¸¸è§ç»´åº¦
                                for dim in [768, 1024, 512, 1536]:
                                    try:
                                        sample_vector = [0.0] * dim
                                        break
                                    except Exception:
                                        continue
                            
                            # æ‰§è¡Œæµ‹è¯•æœç´¢
                            search_results = self.vector_db.search_data(
                                collection=self.collection_name,
                                vector=sample_vector,
                                top_k=1
                            )
                            
                            if search_results:
                                # å¦‚æœèƒ½æœåˆ°ç»“æœï¼Œè¯´æ˜æœ‰æ•°æ®ï¼Œä½†æ— æ³•ç²¾ç¡®è®¡æ•°
                                return {
                                    'name': self.collection_name,
                                    'count': -1,  # ç”¨-1è¡¨ç¤ºæœ‰æ•°æ®ä½†æ— æ³•ç²¾ç¡®è®¡æ•°
                                    'exists': True,
                                    'type': 'Milvus',
                                    'note': 'é›†åˆå­˜åœ¨ä¸”æœ‰æ•°æ®ï¼Œä½†æ— æ³•è·å–ç²¾ç¡®æ•°é‡'
                                }
                            else:
                                # æœç´¢æ— ç»“æœï¼Œå¯èƒ½é›†åˆä¸ºç©º
                                return {
                                    'name': self.collection_name,
                                    'count': 0,
                                    'exists': True,
                                    'type': 'Milvus',
                                    'note': 'é›†åˆå­˜åœ¨ä½†å¯èƒ½ä¸ºç©º'
                                }
                                
                        except Exception as search_error:
                            logger.warning(f"æµ‹è¯•æœç´¢å¤±è´¥: {search_error}")
                            
                            # æœ€åçš„å›é€€ï¼šç¡®è®¤é›†åˆå­˜åœ¨ä½†æ— æ³•è·å–è¯¦ç»†ä¿¡æ¯
                            return {
                                'name': self.collection_name,
                                'count': -1,
                                'exists': True,
                                'type': 'Milvus',
                                'note': 'é›†åˆå­˜åœ¨ä½†æ— æ³•è·å–ç»Ÿè®¡ä¿¡æ¯'
                            }
                            
                except Exception as e:
                    logger.error(f"æ£€æŸ¥Milvusé›†åˆå¤±è´¥: {e}")
                    return {'exists': False, 'error': str(e), 'name': self.collection_name}
            
            # å¯¹äºå…¶ä»–ç±»å‹çš„å‘é‡æ•°æ®åº“ï¼Œä½¿ç”¨é€šç”¨æ–¹æ³•
            else:
                # æ£€æŸ¥é›†åˆæ˜¯å¦å­˜åœ¨ï¼ˆé€‚ç”¨äºæœ‰has_collectionæ–¹æ³•çš„å‘é‡æ•°æ®åº“ï¼‰
                if hasattr(self.vector_db, 'has_collection'):
                    has_collection = self.vector_db.has_collection(self.collection_name)
                    if not has_collection:
                        return {'exists': False, 'name': self.collection_name}
                
                # é€šè¿‡list_collectionsè·å–ä¿¡æ¯
                collections = self.vector_db.list_collections()
                
                for collection in collections:
                    # å°è¯•ä¸åŒå¯èƒ½çš„å±æ€§åç§°è·å–é›†åˆå
                    collection_name = None
                    if hasattr(collection, 'collection_name'):
                        collection_name = collection.collection_name
                    elif hasattr(collection, 'name'):
                        collection_name = collection.name
                    elif hasattr(collection, 'id'):
                        collection_name = collection.id
                    elif isinstance(collection, str):
                        collection_name = collection
                    else:
                        collection_name = str(collection)
                    
                    if collection_name == self.collection_name:
                        # å°è¯•è·å–å®ä½“æ•°é‡
                        num_entities = 0
                        
                        # å°è¯•å¤šç§æ–¹å¼è·å–å®ä½“æ•°é‡
                        count_methods = ['num_entities', 'count', 'size', 'length']
                        for method in count_methods:
                            if hasattr(collection, method):
                                try:
                                    value = getattr(collection, method)
                                    # å¦‚æœæ˜¯æ–¹æ³•ï¼Œè°ƒç”¨å®ƒ
                                    if callable(value):
                                        num_entities = value()
                                    else:
                                        num_entities = value
                                    logger.debug(f"é€šè¿‡ {method} è·å–æ•°é‡: {num_entities}")
                                    break
                                except Exception as e:
                                    logger.debug(f"å°è¯•æ–¹æ³• {method} å¤±è´¥: {e}")
                                    continue
                        
                        return {
                            'name': collection_name,
                            'count': num_entities,
                            'exists': True,
                            'type': vector_db_type
                        }
                
                # å¦‚æœé€šè¿‡list_collectionsæ²¡æ‰¾åˆ°ï¼Œä½†å¯èƒ½é›†åˆä»ç„¶å­˜åœ¨
                return {
                    'name': self.collection_name,
                    'count': -1,
                    'exists': False,  # æ— æ³•ç¡®è®¤å­˜åœ¨
                    'type': vector_db_type,
                    'note': 'æ— æ³•é€šè¿‡list_collectionsæ‰¾åˆ°é›†åˆ'
                }
            
        except Exception as e:
            logger.error(f"è·å–é›†åˆä¿¡æ¯å¤±è´¥: {e}")
            return {
                'exists': False, 
                'error': str(e),
                'name': self.collection_name
            } 