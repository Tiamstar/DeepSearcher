#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
åä¸ºRAGæµæ°´çº¿ - æ•´åˆçˆ¬è™«å’Œå‘é‡åŒ–åŠŸèƒ½
"""

import logging
import asyncio
from typing import Dict, List, Any, Optional
from pathlib import Path

from deepsearcher.configuration import Configuration, init_config

from .config import CrawlerConfig, RAGConfig
from .crawler import HuaweiContentCrawler
from .adapter import HuaweiDeepSearcherAdapter

logger = logging.getLogger(__name__)

class HuaweiRAGPipeline:
    """åä¸ºRAGæµæ°´çº¿ - å®Œæ•´çš„å†…å®¹å¤„ç†æµç¨‹"""
    
    def __init__(self, config_file: str = None):
        """
        åˆå§‹åŒ–RAGæµæ°´çº¿
        
        Args:
            config_file: é…ç½®æ–‡ä»¶è·¯å¾„ï¼Œæš‚æœªä½¿ç”¨ï¼Œä¸ºå°†æ¥æ‰©å±•é¢„ç•™
        """
        self.crawler_config = CrawlerConfig()
        self.rag_config = RAGConfig()
        
        self.crawler = None
        self.adapter = None
        
        # ç¡®ä¿ç›®å½•å­˜åœ¨
        self.crawler_config.ensure_directories()
        
        logger.info("ğŸš€ åä¸ºRAGæµæ°´çº¿åˆå§‹åŒ–å®Œæˆ")
    
    def setup_deepsearcher(self, 
                          llm_model: str = None,
                          embedding_model: str = None,
                          vector_db_config: Dict = None):
        """
        è®¾ç½®DeepSearcheré…ç½®
        
        Args:
            llm_model: LLMæ¨¡å‹åç§°
            embedding_model: åµŒå…¥æ¨¡å‹åç§°  
            vector_db_config: å‘é‡æ•°æ®åº“é…ç½®
        """
        logger.info("âš™ï¸ è®¾ç½®DeepSearcheré…ç½®...")
        
        # ä½¿ç”¨é»˜è®¤é…ç½®æˆ–ä¼ å…¥çš„é…ç½®
        llm_model = llm_model or self.rag_config.DEFAULT_LLM_MODEL
        embedding_model = embedding_model or self.rag_config.DEFAULT_EMBEDDING_MODEL
        vector_db_config = vector_db_config or self.rag_config.DEFAULT_VECTOR_DB_CONFIG
        
        # åˆ›å»ºé…ç½®
        config = Configuration()
        config.set_provider_config("llm", self.rag_config.DEFAULT_LLM_PROVIDER, {"model": llm_model})
        config.set_provider_config("embedding", self.rag_config.DEFAULT_EMBEDDING_PROVIDER, {"model": embedding_model})
        config.set_provider_config("vector_db", self.rag_config.DEFAULT_VECTOR_DB_PROVIDER, vector_db_config)
        
        # åº”ç”¨é…ç½®
        init_config(config)
        
        logger.info(f"âœ… DeepSearcheré…ç½®å®Œæˆ")
        logger.info(f"   LLM: {self.rag_config.DEFAULT_LLM_PROVIDER}/{llm_model}")
        logger.info(f"   åµŒå…¥æ¨¡å‹: {self.rag_config.DEFAULT_EMBEDDING_PROVIDER}/{embedding_model}")
        logger.info(f"   å‘é‡æ•°æ®åº“: {self.rag_config.DEFAULT_VECTOR_DB_PROVIDER}")
    
    def initialize_crawler(self, links_file: str = None) -> HuaweiContentCrawler:
        """åˆå§‹åŒ–çˆ¬è™«"""
        if self.crawler is None:
            logger.info("ğŸ•·ï¸ åˆå§‹åŒ–åä¸ºå†…å®¹çˆ¬è™«...")
            links_file = links_file or self.crawler_config.LINKS_FILE
            self.crawler = HuaweiContentCrawler(links_file, self.crawler_config)
            logger.info("âœ… çˆ¬è™«åˆå§‹åŒ–å®Œæˆ")
        return self.crawler
    
    def initialize_adapter(self, 
                          collection_name: str = None, 
                          content_type: str = "auto") -> HuaweiDeepSearcherAdapter:
        """
        åˆå§‹åŒ–é€‚é…å™¨
        
        Args:
            collection_name: é›†åˆåç§°
            content_type: å†…å®¹ç±»å‹é€‰æ‹© ("auto", "expanded", "basic", "all")
        """
        if self.adapter is None:
            logger.info("ğŸ”„ åˆå§‹åŒ–DeepSearcheré€‚é…å™¨...")
            
            # ç¡®ä¿DeepSearcherå·²é…ç½®
            try:
                from deepsearcher.configuration import vector_db, embedding_model
                if vector_db is None or embedding_model is None:
                    logger.info("âš™ï¸ DeepSearcheræœªé…ç½®ï¼Œæ­£åœ¨è‡ªåŠ¨é…ç½®...")
                    self.setup_deepsearcher()
            except Exception as e:
                logger.warning(f"âš ï¸ æ£€æŸ¥DeepSearcheré…ç½®æ—¶å‡ºé”™: {e}")
                logger.info("âš™ï¸ é‡æ–°é…ç½®DeepSearcher...")
                self.setup_deepsearcher()
            
            collection_name = collection_name or self.rag_config.DEFAULT_COLLECTION_NAME
            
            self.adapter = HuaweiDeepSearcherAdapter(
                collection_name=collection_name,
                chunk_size=self.rag_config.DEFAULT_CHUNK_SIZE,
                chunk_overlap=self.rag_config.DEFAULT_CHUNK_OVERLAP,
                content_type=content_type
            )
            logger.info("âœ… é€‚é…å™¨åˆå§‹åŒ–å®Œæˆ")
        return self.adapter
    
    async def crawl_content(self, 
                           links_file: str = None, 
                           force_recrawl: bool = False) -> bool:
        """
        çˆ¬å–åä¸ºæ–‡æ¡£å†…å®¹
        
        Args:
            links_file: é“¾æ¥æ–‡ä»¶è·¯å¾„
            force_recrawl: æ˜¯å¦å¼ºåˆ¶é‡æ–°çˆ¬å–
        """
        try:
            logger.info("ğŸ•·ï¸ å¼€å§‹çˆ¬å–åä¸ºæ–‡æ¡£å†…å®¹...")
            
            # æ£€æŸ¥æ˜¯å¦å·²æœ‰å†…å®¹ä¸”ä¸å¼ºåˆ¶é‡æ–°çˆ¬å–
            content_file = self.crawler_config.PROCESSED_DATA_DIR / self.crawler_config.OUTPUT_FILE
            if content_file.exists() and not force_recrawl:
                logger.info(f"ğŸ“‚ å‘ç°å·²å­˜åœ¨çš„å†…å®¹æ–‡ä»¶: {content_file}")
                logger.info("ğŸ’¡ å¦‚éœ€é‡æ–°çˆ¬å–ï¼Œè¯·è®¾ç½® force_recrawl=True")
                return True
            
            # åˆå§‹åŒ–çˆ¬è™«
            crawler = self.initialize_crawler(links_file)
            
            # åŠ è½½é“¾æ¥
            urls = crawler.load_links()
            if not urls:
                logger.error("âŒ æ²¡æœ‰æ‰¾åˆ°å¯çˆ¬å–çš„é“¾æ¥")
                return False
            
            # å¼€å§‹çˆ¬å–
            logger.info(f"ğŸ“‹ å¼€å§‹çˆ¬å– {len(urls)} ä¸ªé¡µé¢...")
            crawled_content = await crawler.crawl_content(urls)
            
            if not crawled_content:
                logger.error("âŒ çˆ¬å–å¤±è´¥ï¼Œæ²¡æœ‰è·å¾—ä»»ä½•å†…å®¹")
                return False
            
            # ç”Ÿæˆç»Ÿè®¡ä¿¡æ¯
            stats = crawler.generate_statistics()
            logger.info("ğŸ“Š çˆ¬å–å®Œæˆç»Ÿè®¡:")
            logger.info(f"   âœ… æˆåŠŸ: {stats['total_pages']} é¡µ")
            logger.info(f"   âŒ å¤±è´¥: {stats['failed_pages']} é¡µ")
            logger.info(f"   â© è·³è¿‡: {stats['skipped_pages']} é¡µ")
            logger.info(f"   ğŸ“ æ€»æ–‡æœ¬é•¿åº¦: {stats['total_text_length']:,} å­—ç¬¦")
            logger.info(f"   ğŸ’» æ€»ä»£ç å—: {stats['total_code_blocks']} ä¸ª")
            
            return True
            
        except Exception as e:
            logger.error(f"âŒ çˆ¬å–å†…å®¹å¤±è´¥: {e}")
            return False
    
    def load_to_vector_database(self, 
                               collection_name: str = None,
                               force_new_collection: bool = False,
                               incremental_update: bool = True,
                               batch_size: int = 64,
                               content_type: str = "auto") -> bool:
        """
        åŠ è½½å†…å®¹åˆ°å‘é‡æ•°æ®åº“
        
        Args:
            collection_name: é›†åˆåç§°
            force_new_collection: æ˜¯å¦å¼ºåˆ¶åˆ›å»ºæ–°é›†åˆ
            incremental_update: æ˜¯å¦å¯ç”¨å¢é‡æ›´æ–°ï¼Œä»…å¤„ç†æ–°æ–‡æ¡£
            batch_size: æ‰¹å¤„ç†å¤§å°
            content_type: å†…å®¹ç±»å‹é€‰æ‹© ("auto", "expanded", "basic", "all")
        """
        try:
            logger.info("ğŸ’¾ å¼€å§‹åŠ è½½å†…å®¹åˆ°å‘é‡æ•°æ®åº“...")
            
            # åˆå§‹åŒ–é€‚é…å™¨
            adapter = self.initialize_adapter(collection_name, content_type)
            
            # æ£€æŸ¥å†…å®¹æ–‡ä»¶æ˜¯å¦å­˜åœ¨
            content_file = Path(adapter.content_file)
            if not content_file.exists() and adapter.content_type != "all":
                logger.error(f"âŒ å†…å®¹æ–‡ä»¶ä¸å­˜åœ¨: {content_file}")
                logger.info("ğŸ’¡ è¯·å…ˆè¿è¡Œçˆ¬è™«ç”Ÿæˆå†…å®¹æ–‡ä»¶")
                return False
            
            # å¦‚æœå¼ºåˆ¶é‡å»ºé›†åˆï¼Œåˆ™ç¦ç”¨å¢é‡æ›´æ–°
            if force_new_collection:
                incremental_update = False
                logger.info("ğŸ”„ å¼ºåˆ¶é‡å»ºé›†åˆï¼Œç¦ç”¨å¢é‡æ›´æ–°")
            
            # åŠ è½½åˆ°å‘é‡æ•°æ®åº“
            success = adapter.load_to_vector_database(
                force_new_collection=force_new_collection,
                incremental_update=incremental_update,
                batch_size=batch_size
            )
            
            if success:
                # æ˜¾ç¤ºé›†åˆä¿¡æ¯
                collection_info = adapter.get_collection_info()
                if collection_info.get('exists'):
                    logger.info(f"ğŸ“š é›†åˆä¿¡æ¯: {collection_info['name']} ({collection_info.get('count', 0)} ä¸ªæ–‡æ¡£)")
            
            return success
            
        except Exception as e:
            logger.error(f"âŒ åŠ è½½åˆ°å‘é‡æ•°æ®åº“å¤±è´¥: {e}")
            return False
    
    def search(self, 
              query: str, 
              top_k: int = 5,
              content_type: str = None,
              collection_name: str = None) -> List[Dict]:
        """
        æœç´¢åä¸ºæ–‡æ¡£
        
        Args:
            query: æœç´¢æŸ¥è¯¢
            top_k: è¿”å›ç»“æœæ•°é‡
            content_type: å†…å®¹ç±»å‹è¿‡æ»¤ ('text', 'code')
            collection_name: é›†åˆåç§°
        """
        try:
            # åˆå§‹åŒ–é€‚é…å™¨
            adapter = self.initialize_adapter(collection_name)
            
            # æ‰§è¡Œæœç´¢
            results = adapter.search_huawei_docs(
                query=query,
                top_k=top_k,
                content_type=content_type
            )
            
            return results
            
        except Exception as e:
            logger.error(f"âŒ æœç´¢å¤±è´¥: {e}")
            return []
    
    def search_with_rag(self, 
                       query: str, 
                       top_k: int = 5,
                       content_type: str = None,
                       collection_name: str = None,
                       use_chain_of_rag: bool = False) -> List[Dict]:
        """
        ä½¿ç”¨DeepSearcherçš„é«˜çº§RAGåŠŸèƒ½æœç´¢åä¸ºæ–‡æ¡£
        
        Args:
            query: æœç´¢æŸ¥è¯¢
            top_k: è¿”å›ç»“æœæ•°é‡
            content_type: å†…å®¹ç±»å‹è¿‡æ»¤ ('text', 'code')
            collection_name: é›†åˆåç§°
            use_chain_of_rag: æ˜¯å¦ä½¿ç”¨ChainOfRAGè¿›è¡Œå¤šæ­¥æ¨ç†æœç´¢
        """
        try:
            # åˆå§‹åŒ–é€‚é…å™¨
            adapter = self.initialize_adapter(collection_name)
            
            # ä½¿ç”¨DeepSearcherçš„é«˜çº§RAGåŠŸèƒ½
            from deepsearcher.configuration import llm, embedding_model, vector_db
            from deepsearcher.agent.naive_rag import NaiveRAG
            from deepsearcher.agent.chain_of_rag import ChainOfRAG
            from deepsearcher.agent.deep_search import DeepSearch
            
            if use_chain_of_rag:
                # ä½¿ç”¨ChainOfRAGè¿›è¡Œå¤šæ­¥æ¨ç†æœç´¢
                rag_agent = ChainOfRAG(
                    llm=llm,
                    embedding_model=embedding_model,
                    vector_db=vector_db,
                    max_iter=3,
                    early_stopping=True
                )
                logger.info("ğŸ”— ä½¿ç”¨ChainOfRAGè¿›è¡Œå¤šæ­¥æ¨ç†æœç´¢")
            else:
                # ä½¿ç”¨DeepSearchè¿›è¡Œæ·±åº¦æœç´¢
                rag_agent = DeepSearch(
                    llm=llm,
                    embedding_model=embedding_model,
                    vector_db=vector_db,
                    top_k=top_k
                )
                logger.info("ğŸ” ä½¿ç”¨DeepSearchè¿›è¡Œæ·±åº¦æœç´¢")
            
            # æ‰§è¡ŒRAGæŸ¥è¯¢
            answer, retrieved_results, token_usage = rag_agent.query(query, top_k=top_k)
            
            # è½¬æ¢ç»“æœæ ¼å¼
            formatted_results = []
            for result in retrieved_results:
                formatted_result = {
                    'title': result.metadata.get('title', 'æœªçŸ¥æ ‡é¢˜'),
                    'url': result.metadata.get('url', 'æ— é“¾æ¥'),
                    'content': result.text,
                    'content_type': result.metadata.get('content_type', 'text'),
                    'score': result.score,
                    'reference': result.reference,
                    'rag_answer': answer  # æ·»åŠ RAGç”Ÿæˆçš„ç­”æ¡ˆ
                }
                formatted_results.append(formatted_result)
            
            logger.info(f"âœ… RAGæœç´¢å®Œæˆï¼Œä½¿ç”¨äº† {token_usage} tokens")
            return formatted_results
            
        except Exception as e:
            logger.error(f"âŒ RAGæœç´¢å¤±è´¥: {e}")
            # é™çº§åˆ°æ™®é€šæœç´¢
            logger.info("ğŸ”„ é™çº§åˆ°æ™®é€šå‘é‡æœç´¢")
            return self.search(query, top_k, content_type, collection_name)
    
    async def run_full_pipeline(self, 
                         links_file: str = None,
                         collection_name: str = None,
                         force_recrawl: bool = False,
                         force_new_collection: bool = False,
                         batch_size: int = 64) -> bool:
        """
        è¿è¡Œå®Œæ•´çš„RAGæµæ°´çº¿
        
        Args:
            links_file: é“¾æ¥æ–‡ä»¶è·¯å¾„
            collection_name: é›†åˆåç§°
            force_recrawl: æ˜¯å¦å¼ºåˆ¶é‡æ–°çˆ¬å–
            force_new_collection: æ˜¯å¦å¼ºåˆ¶åˆ›å»ºæ–°é›†åˆ
            batch_size: æ‰¹å¤„ç†å¤§å°
        """
        try:
            logger.info("ğŸš€ å¼€å§‹è¿è¡Œå®Œæ•´çš„åä¸ºRAGæµæ°´çº¿...")
            
            # 1. è®¾ç½®DeepSearcher
            self.setup_deepsearcher()
            
            # 2. çˆ¬å–å†…å®¹
            logger.info("\n" + "="*50)
            logger.info("ğŸ“‹ ç¬¬1æ­¥: çˆ¬å–åä¸ºæ–‡æ¡£å†…å®¹")
            logger.info("="*50)
            
            crawl_success = await self.crawl_content(
                links_file=links_file,
                force_recrawl=force_recrawl
            )
            
            if not crawl_success:
                logger.error("âŒ çˆ¬å–é˜¶æ®µå¤±è´¥ï¼Œåœæ­¢æµæ°´çº¿")
                return False
            
            # 3. åŠ è½½åˆ°å‘é‡æ•°æ®åº“
            logger.info("\n" + "="*50)
            logger.info("ğŸ“‹ ç¬¬2æ­¥: åŠ è½½åˆ°å‘é‡æ•°æ®åº“")
            logger.info("="*50)
            
            load_success = self.load_to_vector_database(
                collection_name=collection_name,
                force_new_collection=force_new_collection,
                batch_size=batch_size
            )
            
            if not load_success:
                logger.error("âŒ å‘é‡åŒ–é˜¶æ®µå¤±è´¥ï¼Œåœæ­¢æµæ°´çº¿")
                return False
            
            logger.info("\n" + "="*50)
            logger.info("ğŸ‰ åä¸ºRAGæµæ°´çº¿å®Œæˆ!")
            logger.info("="*50)
            logger.info("âœ… æ‰€æœ‰æ­¥éª¤æˆåŠŸå®Œæˆ")
            logger.info("ğŸ’¡ ç°åœ¨å¯ä»¥ä½¿ç”¨ search() æ–¹æ³•è¿›è¡Œæ–‡æ¡£æœç´¢")
            
            return True
            
        except Exception as e:
            logger.error(f"âŒ æµæ°´çº¿æ‰§è¡Œå¤±è´¥: {e}")
            return False

    def run_full_pipeline_sync(self, 
                              links_file: str = None,
                              collection_name: str = None,
                              force_recrawl: bool = False,
                              force_new_collection: bool = False,
                              batch_size: int = 64) -> bool:
        """
        è¿è¡Œå®Œæ•´çš„RAGæµæ°´çº¿ (åŒæ­¥ç‰ˆæœ¬)
        
        Args:
            links_file: é“¾æ¥æ–‡ä»¶è·¯å¾„
            collection_name: é›†åˆåç§°  
            force_recrawl: æ˜¯å¦å¼ºåˆ¶é‡æ–°çˆ¬å–
            force_new_collection: æ˜¯å¦å¼ºåˆ¶åˆ›å»ºæ–°é›†åˆ
            batch_size: æ‰¹å¤„ç†å¤§å°
        """
        return run_pipeline_async(
            self.run_full_pipeline,
            links_file=links_file,
            collection_name=collection_name,
            force_recrawl=force_recrawl,
            force_new_collection=force_new_collection,
            batch_size=batch_size
        )
    
    def get_status(self) -> Dict[str, Any]:
        """è·å–æµæ°´çº¿çŠ¶æ€"""
        status = {
            'crawler_initialized': self.crawler is not None,
            'adapter_initialized': self.adapter is not None,
            'content_file_exists': False,
            'collection_exists': False,
            'collection_info': {}
        }
        
        # æ£€æŸ¥å†…å®¹æ–‡ä»¶
        content_file = self.crawler_config.PROCESSED_DATA_DIR / self.crawler_config.OUTPUT_FILE
        status['content_file_exists'] = content_file.exists()
        if content_file.exists():
            status['content_file_path'] = str(content_file)
        
        # æ£€æŸ¥é›†åˆä¿¡æ¯
        if self.adapter:
            try:
                collection_info = self.adapter.get_collection_info()
                status['collection_exists'] = collection_info.get('exists', False)
                status['collection_info'] = collection_info
            except:
                pass
        
        return status
    
    def print_status(self):
        """æ‰“å°æµæ°´çº¿çŠ¶æ€"""
        status = self.get_status()
        
        logger.info("ğŸ“Š åä¸ºRAGæµæ°´çº¿çŠ¶æ€:")
        logger.info(f"   ğŸ•·ï¸ çˆ¬è™«: {'âœ…' if status['crawler_initialized'] else 'âŒ'}")
        logger.info(f"   ğŸ”„ é€‚é…å™¨: {'âœ…' if status['adapter_initialized'] else 'âŒ'}")
        logger.info(f"   ğŸ“„ å†…å®¹æ–‡ä»¶: {'âœ…' if status['content_file_exists'] else 'âŒ'}")
        logger.info(f"   ğŸ“š å‘é‡é›†åˆ: {'âœ…' if status['collection_exists'] else 'âŒ'}")
        
        if status['collection_exists']:
            info = status['collection_info']
            logger.info(f"       é›†åˆåç§°: {info.get('name', 'Unknown')}")
            logger.info(f"       æ–‡æ¡£æ•°é‡: {info.get('count', 0):,}")
        
        if status['content_file_exists']:
            logger.info(f"       å†…å®¹æ–‡ä»¶: {status.get('content_file_path', 'Unknown')}")

    def list_content_files(self) -> Dict[str, Any]:
        """åˆ—å‡ºæ‰€æœ‰å¯ç”¨çš„å†…å®¹æ–‡ä»¶"""
        try:
            # åˆ›å»ºä¸€ä¸ªä¸´æ—¶é€‚é…å™¨æ¥è·å–æ–‡ä»¶åˆ—è¡¨
            temp_adapter = HuaweiDeepSearcherAdapter()
            return temp_adapter.list_available_content_files()
        except Exception as e:
            logger.error(f"âŒ åˆ—å‡ºå†…å®¹æ–‡ä»¶å¤±è´¥: {e}")
            return {}


# å¼‚æ­¥è¿è¡ŒåŠ©æ‰‹å‡½æ•°
def run_pipeline_async(pipeline_func, *args, **kwargs):
    """è¿è¡Œå¼‚æ­¥æµæ°´çº¿çš„åŠ©æ‰‹å‡½æ•°"""
    try:
        loop = asyncio.get_event_loop()
    except RuntimeError:
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
    
    return loop.run_until_complete(pipeline_func(*args, **kwargs)) 